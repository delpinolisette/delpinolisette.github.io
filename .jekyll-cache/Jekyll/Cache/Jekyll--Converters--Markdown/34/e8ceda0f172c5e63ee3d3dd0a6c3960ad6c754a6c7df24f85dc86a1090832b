I";)<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p><em>June 2022 edits: I wrote this page 2 years during my Advanced Linear Algebra course at Penn ago and gave gotten much better at math, exposition, and math typesetting since then. I am slowly working through it to make it more readable</em></p>

<ul>
  <li><a href="#composition-of-linear-maps">Composition of Linear Maps</a>
    <ul>
      <li><a href="#properties-of-compsition-of-linear-maps">Properties of Compsition of Linear Maps</a></li>
    </ul>
  </li>
  <li><a href="#dual-spaces-and-isomorphism">Dual Spaces and Isomorphism</a>
    <ul>
      <li><a href="#lectures-12-13">Lectures 12-13</a></li>
      <li><a href="#lecture-14-row-reduction">Lecture 14: Row Reduction</a></li>
      <li><a href="#chapter-1">Chapter 1</a></li>
      <li><a href="#chapter-2">Chapter 2</a>
        <ul>
          <li><a href="#section-23--analyzing-the-pivots">Section 2.3 : Analyzing the Pivots:</a></li>
          <li><a href="#section-27--fundamental-subspaces-and-rank">Section 2.7 : Fundamental Subspaces and Rank:</a></li>
        </ul>
      </li>
      <li><a href="#chapter-3">Chapter 3</a></li>
      <li><a href="#chapter-4">Chapter 4</a></li>
      <li><a href="#chapter-9">Chapter 9</a></li>
      <li><a href="#chapter-8-dual-spaces-and-tensors">Chapter 8: Dual Spaces and Tensors</a>
        <ul>
          <li><a href="#section-81--dual-spaces">Section 8.1 : Dual Spaces</a></li>
          <li><a href="#extra-notesdefs-to-categorize-later">Extra notes/defs to categorize later</a></li>
        </ul>
      </li>
      <li><a href="#determinants">Determinants</a>
        <ul>
          <li><a href="#exercises">exercises</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="composition-of-linear-maps">Composition of Linear Maps</h1>

<p>Recall that if $X,Y,Z$ are all sets, and $f: X \mapsto Y$ and $g: Y \mapsto Z $ if we have a mapping $f$ from $X$ to $Y$ and a mapping $g$ from $Y$ to $Z$, we can form a composite map:</p>

<ul>
  <li>
    <p>$(g \circ f): X \mapsto Z$ that is defined by $g \circ f = g(f(x)) \) for all \( x \in X \)</p>
  </li>
  <li>we already used the notion of composition to define invertible/bijective maps of sets.</li>
  <li>composition also preserves the property of maps being linear!! <em>verycool</em> we state it formally and prove it:</li>
  <li><strong><em>Claim</em></strong>: The composition of two linear maps is also linear. Let U, V, W be \( \mathbb{F} \) vector spaces.
    <ul>
      <li>Let these be linear maps:</li>
      <li>\( T: U \mapsto V \) and \( S: V \mapsto W \)</li>
      <li>the compositon of these is also linear:</li>
      <li>\( S \circ T : U \mapsto W \)</li>
    </ul>
  </li>
  <li><em>proof</em>: Let \( x,y \in V \), \( c \in K \). Then:
    <ul>
      <li>\( S \circ T(x+y) = S(T(x+y)) = S(T(x) + T(y)) = S(T(y)) + S(T(y)) = S \circ T(x) + S \circ T(y) \)</li>
      <li>also, \( S \circ T(cx) = S(T(cx)) = S(cT(x)) = cS(T(x)) = cS \circ T(x) \)</li>
      <li>QED</li>
    </ul>
  </li>
  <li>Thus the operation of compositions gives yet another way of constructing new linear maps out of known ones.</li>
  <li>Under our dictionary that uses bases to convert linear maps to matrices and matrices back to linear maps (QUESTION: lets see an example)
    <ul>
      <li>=&gt; the natural operations on linear maps that produce new linear maps just become the natural operations of matrices (that produce new matrices?)</li>
      <li><strong><em>Theorem</em></strong>: Let U, V, W, be \( \mathbb{F} \) -vector spaces with bases:
        <ul>
          <li>\( \mathbb{E} = { e_{1},…….,e_{n} } \subset U \)</li>
          <li>\( \mathbb{F} = { f_{1},…….,f_{n} } \subset V \)</li>
          <li>\( \mathbb{G} = { g_{1},…….,g_{n} } \subset W \)</li>
        </ul>
      </li>
      <li>(1) : If \( T_{1}: U \mapsto V, T_{2}: U \mapsto V  \) are linear mappings, \( c \in \mathbb{F} \)
        <ul>
          <li>and \( A_{1}, A_{2} \in Mat_{mxn}\mathbb{F}\) are matrices of \( T_{1}, T_{2} \) in the bases \( \mathbb{E}, \mathbb{F} \)</li>
          <li>then matrices of :
            <ul>
              <li>\( T_{1} + T_{2}: U \mapsto V \) == \( A_{1} + A_{2} \)</li>
              <li>\( cT_{1}: U \mapsto V \) == \( cA_{1} \)</li>
            </ul>
          </li>
          <li>in our bases \( \mathbb{E} \) and \( \mathbb{F} \)</li>
        </ul>
      </li>
      <li>(2) :  (Linear mapping composition corresponds to matrix multiplcation): if \( T: U \mapsto V \) and \( S: V \mapsto W \) are linear mappings and
        <ul>
          <li>T has an (m x n) matrix A with bases E (from U) and F (from V) [bases from domain and codomain]</li>
          <li>S has a (k x m) matrix B with bases F (from V) and G (from W) [bases from domain and codomain]</li>
        </ul>
      </li>
      <li>Then \( S \circ T \) has an (k x n) matrix B*A in the bases E (from domain U) and G (from codomain W)</li>
      <li><em>proof</em>:
        <ul>
          <li>(1) follows from the definition of the matrix of a linear map</li>
          <li>(2) Consider the linear map \( f:= S \circ T: U \mapsto W \) and let C be the matrix of of that operator \( S \circ T \) in the bases E (from domain U) and G (from codomain W)
            <ul>
              <li>every entry of C \( c_{pq} \) is given by:</li>
              <li>\( c_{qp} = \) the q-th coordinate of the vector \( f_{e_{p}} \) in the basis \( \mathbb{G} = (g_{1}…g_{n}) \)</li>
              <li>we can actually compute each entry \( c_{qp} = f_{e_{p}} \) directly:</li>
              <li>\( f_{e_{p}} = S \circ T(e_{p})  \)</li>
              <li>= \( S(T(e_{p})) \)</li>
              <li>= \( S(A_{1p}f_{1} +…+ A_{mp}f_{m}) \) QUESTION: where does m come from?</li>
              <li>= \( \sum_{r=1}^{m} A_{rp} (S_{f_{r}}) \)</li>
              <li>= \( \sum_{r=1}^{m} A_{rp} (\sum_{q=1}^{k} B_{qr} g_{q})\)</li>
              <li>= \( \sum _{1}^{m} (\sum_{1}^{m} B_{qr}A_{rp}) g_q{} \)</li>
              <li>so we have \( c_{qp} = \sum_{1}^{m} B_{qr}A_{rp} \)</li>
              <li>= (row q of B)(column p of A) = \( (BA)_{qp} \)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Example 1 : (General Reflections)
        <ul>
          <li>Let \( L \in \mathbb(R)^{2}\) be any line through the origin and let the transformaition \(s_{L} : R^{2} \mapsto R^{2}\) be the reflection across L</li>
          <li>so \(s_{L}\) send a point in R^{2} to its mirrror image, where them mirror is L</li>
          <li><strong><em>Claim</em></strong> : \(s_{L}\) is a linear map.</li>
          <li><em>proof</em>: you can prove this directly using difficult geometric constructions, or you can use the fact that compositions of linear maps are linear:</li>
          <li>S1: show that the reflection across the x-axis is linear:
            <ul>
              <li>\(s = s_{x-axis} : \mathbb{R^{2} \mapsto \mathbb{R^{2}}}\)</li>
              <li>\s(x, y)^{t} = (x -y)^{t}\) for all vectors x,y</li>
              <li><img src="delpinolisette.github.io/img/reflectx.png" alt="reflection over x axis" /></li>
              <li>reflection over x axis matrix ==</li>
              <li>A = \(
          \begin{bmatrix}
          1 &amp; 0  \<br />
          0 &amp; -1  \<br />
          \end{bmatrix}<br />
          \)</li>
            </ul>
          </li>
          <li>S2: notice that reflection across L can be viewed as a composition of three linear maps:
            <ul>
              <li>
                <ol>
                  <li>rotate the plane so that L becomes the x-axis</li>
                </ol>
              </li>
              <li>
                <ol>
                  <li>reflect the plane across the x axis</li>
                </ol>
              </li>
              <li>
                <ol>
                  <li>rotate the plane so that x-axis becomes L</li>
                </ol>
              </li>
            </ul>
          </li>
          <li>each of these is linear, so \(s_{L}\) is linear.</li>
          <li>S3: in concrete terms, \(\theta : \) measured in radians, counter closckwise direction, Then,
            <ul>
              <li>\(s_{L} = rot_{\theta} \circ s \circ rot_{-\theta}\)</li>
              <li>use the previous theorem, and we can get matrices for these transformations:</li>
              <li>\(A_{rot_{\theta}}\) = \(
          \begin{bmatrix}
          cos(\theta) &amp; -sin(\theta)  \<br />
          sin(\theta) &amp; cos(\theta)  \<br />
          \end{bmatrix}<br />
          \)</li>
              <li>\(A_{rot_{\theta}}\) = \(
          \begin{bmatrix}
          cos(\theta) &amp; sin(\theta)  \<br />
          -sin(\theta) &amp; cos(\theta)  \<br />
          \end{bmatrix}<br />
          \)</li>
              <li>\(A_{s}\) = reflection over x axis matrix</li>
              <li>EXERCISE: derive these matrices</li>
              <li>and so \(A_{s_{L}}\) = multiplication/composition of the three: = \(
          \begin{bmatrix}
          cos(2\theta) &amp; sin(2\theta)  \<br />
          sin(2\theta) &amp; -cos(2\theta)  \<br />
          \end{bmatrix}<br />
          \)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>EXERCISE: Compute the matrix of the reflection over the line L with respect to the line L: y =7x in the standard basis of \(R^{2}\)</li>
    </ul>
  </li>
</ul>

<h2 id="properties-of-compsition-of-linear-maps">Properties of Compsition of Linear Maps</h2>

<ul>
  <li>(1): if
    <ul>
      <li>\( T \in L(V,W)\)</li>
      <li>\( S \in L(U,V)\)</li>
      <li>then \(T \circ S \in L(U,W)\)</li>
    </ul>
  </li>
  <li>(2) Composition is a linear map in each argument. if
    <ul>
      <li>\( S \in L(U,V) \)</li>
      <li>then the map \( S \circ (-): L(V,W) \mapsto L(U,W) \) (no entiendo? QUESTION) is a linear map between the two vector spaces L(V,W), L(U,V)</li>
      <li><em>example</em>: if \(a, b \in \mathbb{F}\), \(T_{1}, T_{2} \in L(V,W)\)</li>
      <li>then \(S \circ (aT_{a} + b_{T_{2}})\)</li>
      <li>= \( aS \circ T_{1} + bS \circ T_{2} \).</li>
      <li><em>check</em>: we can check this directly! just check both sides are equal to the same thing when evaluated on any vect \(u \in U\) (because U is domain of S? That’s where we start)</li>
      <li>from the definition of addition and scaling in the vector space of linear maps, <a href="https://math.stackexchange.com/questions/2381942/the-set-of-all-linear-maps-tv-w-is-a-vector-space/2381955">here it is!</a></li>
      <li>= \(S \circ (aT_{1}+bT_{2})(u)\)</li>
      <li>= \(S \circ ((aT_{1}+bT_{2})(u))\)</li>
      <li>= \(S \circ (aT_{1}(u) + bT_{2}(u))\)</li>
      <li>= \( aS(T_{1}(u)) + bS(T_{2}(u))) \)</li>
      <li>= \( a S \circ T_{1}(u) + b S \circ T_{2}(u)\)</li>
      <li>QED</li>
      <li>QUESTION:  CHECK WHETHER LINEAR MAPS IN QUESTION ARE CORRECT: THEY MIGHT BE FLIPPED IN THE NOTES! for this next one too</li>
      <li>similarly: \(T \in L(V,W)\), check directly that \((-) \circ T : L(U,V) \mapsto L(U,W)\) is linear
        <ul>
          <li>because \((aS_{1} + bS_{2}) \circ T = aS_{1} \circ T + b S_{2} \circ T\) for all a,b scalars and \(S_{1}, S_{2} \in L(U,V)\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>(3): Inverse maps of linear maps are linear.
    <ul>
      <li>Suppose \(T: V \mapsto W\) is al inear map which is bijective (is invertible as a map of sets + is injective and surjective + is one to one and onto)</li>
      <li>consider the inverse map \(T_{-1}: W \mapsto V\),</li>
      <li>then \(T_{-1}\) is also linear.</li>
      <li><em>proof</em>: let \(y_{1}, y_{2} \in W\) - the domain of inverse map, let \(a_{1}, a_{2} be two scalars in \mathbb{F}\)
        <ul>
          <li>WTS: \( T_{-1}(a_{1}y_{1} + a_{2}y_{2}) = a_{1}T^{-1}(y_{1}) + a_{2}T^{-1}(y_{2}) \)</li>
          <li>assign some names: \( x_{1} = T^{-1}(y_{1}) \in V \) and \(x_{2} = T^{-1}(y_{2}) \in V \) - V =codomain of \(T^{-1}\)</li>
          <li>then the RHS \( a_{1}T^{-1}(y_{1}) + a_{2}T^{-1}(y_{2}) \) is equal to \(a_{1}x_{1} + a_{2}x_{2} \in V\)</li>
          <li>Let’s evaluate the original map T on this vector. Remember T is linear and we get:</li>
          <li>\( T(a_{1}x_{1} + a_{2}x_{2}) = a_{1}T(x_{1}) + a_{2}T(x_{2}) \)</li>
          <li>= \( a_{1}T \circ T^{-1}(y_{1}) + a_{2}T \circ T^{-1}(y_{2})\)</li>
          <li>= \( a_{1}y_{1} + a_{2}y_{2} \)</li>
          <li>Thus, \(T(a_{1}x_{1} + a_{2}x_{2}) = a_{1}y_{1} + a_{2}y_{2}\)</li>
          <li>Now let’s evaluate \(T^{-1}\) on both sides of this identity. Literally just:</li>
          <li>\(T^{-1} \circ T(a_{1}x_{1} + a_{2}x_{2})\) = \( T^{-1} (a_{1}y_{1} + a_{2}y_{2})\)</li>
          <li>=&gt; \( a_{1}x_{1} + a_{2}x_{2}\) = \( T^{-1} (a_{1}y_{1} + a_{2}y_{2})\)</li>
          <li>\( a_{1}T^{-1}(y_{1}) + a_{2}T^{-1}(y_{2})\) = \( T^{-1} (a_{1}y_{1} + a_{2}y_{2})\)</li>
          <li>we have shown the linearity of the inverse map \(T^{-1}\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>isomorphism</strong> (def): the linear map \(T : V \mapsto W\) is an <em>isomorphism</em> if there exists a linear map \(S: W \mapsto V\) s.t \(S \circ T = id_{v}\) and \(T \circ S = id_{v}\) (also think about corresponding definition in terms of matrices)<a href="https://math.stackexchange.com/questions/441758/what-does-isomorphic-mean-in-linear-algebra">more info</a>
    <ul>
      <li>note: we checked above that a linear map is an isomorphism iff it is a bijection</li>
      <li>note: we also checked that the inverse linear map is simply the inverse set theoretic map.</li>
    </ul>
  </li>
  <li><strong>isomorphism</strong> (def): two vector spaces V,W over \(\mathbb{F}\) are isomorphic if there exists a linear map \(T: V \mapsto W\) which is an isomorphism (so if it is bijective).
    <ul>
      <li>isomorphic means “has same shape”. useful for turning undamiliar algebraic objects into familiar ones, making them easier to work with.</li>
      <li>note: as with sets we will not distinguish isomorphic (vector) spaces.
        <ul>
          <li>the rational is tht using the isomorphism and its inverse we can transport any property of V to W and back again</li>
          <li>these preperties and feauees that only involce teh addition and scaling of vectors are matched in V and W via the isomorphism and its inverse.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Examples of Isomorphisms and Isomorphic Spaces</strong>:</li>
  <li><a href="https://en.wikibooks.org/wiki/Linear_Algebra/Definition_and_Examples_of_Isomorphisms">more examples</a></li>
  <li>Example (1): (iso that sends linear combination vector to its coordinates and back)If V is a finite dimensional vector space over \(\mathbb{F}\)
    <ul>
      <li>then every choice of a basis \(\mathbb{B} = \{ b_{1},..,b_{n} \}\) gives you an isomorphism, namely: \( [-]_{\mathbb{B}} : V \mapsto \mathbb{F} ^{n}\) between V and coordinate n space</li>
      <li>so what does this map do?</li>
      <li>\([-]_{\mathbb{B}}\) assigns to each \( v \in V\) <em>the column vector of its coordinates</em> in the basis \(\mathbb{B}\)</li>
      <li>what this means is that each \( v \in V\) has a unique representation as \(v = \sum_{i=1}^{n} x_{i}b_{i}\) with \( (x_{1},..,x_{n}) \in V, b_{1}…b_{n} \in \mathbb{B} \) (in the vector space V!!)</li>
      <li>this vector gets sent to a column vector of the coordinates from the field. \( [v]_{B} := (x_{1}…x_{n})^{T} \)</li>
      <li>conversely, the inverse map \([-]^{-1} : \mathbb{F} ^{n} \mapsto V\) is also easy to write explicitly. Sends a column vector of coordinates from the scalar field to the linear combination representation of the vector v.</li>
    </ul>
  </li>
  <li><em>Notation:</em> we can write and see the linear combination \(\sum_{i}^{n} x_{i}b_{i}\) as a formal matrix product \(\mathbb{B} \cdot x\) where
    <ul>
      <li>\(\mathbb{B} = (b_{1},…,b_{n})\) == the row vector of vectors (in the basis)</li>
      <li>\(x = (x_{1},…,x_{n})^{t}\) is the column vector of coordinates in \(\mathbb{F} ^{n}\)</li>
      <li>with this new notation, new view, we can look at the inverse map of \([-]_{B}: V \mapsto F^{n}\) as the “matrix multiplication by B map”: \(B \cdot (-) : \mathbb{F} ^{n} \mapsto V\)</li>
    </ul>
  </li>
  <li><em>further notes on example 1</em>: note that the isomorphism [-] depends on your choice of a basis.
    <ul>
      <li>different bases will give rise to different isomophism.</li>
      <li>another way to descrube \([-]_{\mathbb{B}} : V \mapsto \mathbb{F} ^{n}\) is to say that it is the unique linear map that sends the basis B of V to the standard basis \(E = \{e_{1},…,e_{n}\} \) of \(\mathbb{F} ^{n}\).
        <ul>
          <li>so the mapping \([-]_{\mathbb{B}}\) is the unique linear map for which \( [b_{i}]_{\mathbb{B}} = e_{i}\) for i = 1,…,n</li>
          <li>in vector notation : \( [\mathbb{B}]_{\mathbb{B}} = E \)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>(2): Let V,W be vector spaces over \(\mathbb{F}\) with bases
    <ul>
      <li>\(E = \{ e_{1},…,e_{n} \} \subset V\)</li>
      <li>\(F = \{ f_{1},…,f_{n} \} \subset W\)</li>
      <li>now given a linear map \(T: V \mapsto W\) let \(A_{T} \in Mat(mxn)(\mathbb{F})\) be its matrix (T’s matrix) in the bases E and F.</li>
      <li>Then the assignment \(A_{(-)}: L(V,W) \mapsto  Mat(mxn)(\mathbb{F}) :: T \mapsto A_{T}\) is a linear map.
        <ul>
          <li>this follows from part (1) of the first theorem above.</li>
        </ul>
      </li>
      <li>And \(A_{(-)}\) is also an isomorphism. (it’s bijective)
        <ul>
          <li><em>check</em>:
            <ul>
              <li>first, by definition, \( A_{T} \) is the unique matrix such that \( T(E) = T(e_{1} + .. + T(e_{n})) = F \cdot A_{T} \)
                <ul>
                  <li>QUESTION: why is this true? are \(T(e_{1}, …)\) are column vectors?</li>
                </ul>
              </li>
              <li>This fact guides us on how to compute values of T in terms of \(A_{T}\): (Here’s how):
                <ul>
                  <li>if \(v \in V\) and we have \(x \in F^{n}\) := vector of coordinates of v in the basis E,</li>
                  <li>then \(v = Ex\)</li>
                  <li>by linearity of T we have:</li>
                  <li>\( T(v) = T(Ex) = T(E) \cdot x = F \cot A_{T}x\)</li>
                  <li>recalling that \(x = [v]_{E} \in F^{n}\) is the vector of the cordinates of v in the basis E, we now have a formula for T in terms of A_{T}, its matrix:
                    <ul>
                      <li>\(T(v) = F A_{T}[v]_{E}\)</li>
                    </ul>
                  </li>
                  <li>this derivation also gives us the inverse of the map \(A_{(-)} : T \mapsto A_{T}\),
                    <ul>
                      <li>\(A_{(-)} ^{-1} : Mat(mxn)(\mathbb{F}) \mapsto L(V,W) :: A \mapsto FA[-]_{\mathbb{E}}\)</li>
                      <li>QUESTION: so what does this actually look like? need examples</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Example (3): variant of example 1 (T that sends bases to bases is an iso)</li>
  <li>Let V,W be vector spaces over \(\mathbb{F}\) with the bases:
    <ul>
      <li>\(E = \{ e_{1},…,e_{n} \} \subset V \)</li>
      <li>\(F = \{ f_{1},…,f_{n} \} \subset W \)</li>
    </ul>
  </li>
  <li>Then the unique linear map \(T: V \mapsto W\) which sends the basis E to the basis F is an isomorphism!</li>
  <li><em>Proof</em>: Let T be as defined above with bases E and F, for which \(T(E) = F\)</li>
  <li>and let \(S: W \mapsto V\) be another such that \(S(F) = E\)</li>
  <li>both T and S exist and are unique thanks to the theorem we proved above and lectures 7,8.</li>
  <li>Then \(S \circ T : V \mapsto V\) is a linear map such that
    <ul>
      <li>\(S \circ T(E) = S(T(E)) = S(F) = E\)</li>
    </ul>
  </li>
  <li>But we already have such a map that maps from V to V and sends E to E. It’s the identity map! \(id_{v}\)</li>
  <li>by the uniqueness part of the above theorem and the last lecture, we get that this \(S \circ T = id_{v}\), they are one and the same.</li>
  <li>the same argument applies to \(T \circ S\) on F, it equals \(id_{w}\)</li>
  <li><strong>Corollary</strong>: Let V,W be finite dimensional vector spaces over F. Then
    <ul>
      <li>(V and W are isomorphic) iff (dim V = dim W) <em>nice</em></li>
      <li><em>proof</em>: =&gt; follows from isomorphism. look:
        <ul>
          <li>If \(T: V \mapsto W\) is an isomorphism then for any basis B of V the vectors \(T(B) \subset W\) form the basis of W (important point!)</li>
          <li>We can check this directly:
            <ul>
              <li>if \(w \in W\) then we can consider \(v = T^{-1}(w) \in V\)</li>
              <li>by definition, \(T(v) = T(T^{-1})(w) = w\).</li>
              <li>but, \(v \in (V = span(B))\)</li>
              <li>So by the linearity of T we have
                <ul>
                  <li>\(w = (T(v) \in span(T(B)))\) QUESTION: is spanT(B) = W? oh wait thats the conclusion we are trying to reach.</li>
                </ul>
              </li>
              <li>This shows that T(B) is a generating set of W</li>
              <li>But if \(B = \{ b_{1}…b_{n} \} \) and \( \{c_{1},..,c_{n}\} \) are scalars in the field such that the linear combination of these with the bases vectors is in W and equals 0, or \( c_{1}T(b_{1})+…+c_{n}T(b_{n}) = \vec{0} \in W\)</li>
              <li>then we can apply \(T^{-1}\) to both sides and get:</li>
              <li>\( T^{-1}(\vec{0}) = T^{-1}(\sum_{i=1}^{n} c_{i}T(b_{i})) \)</li>
              <li>= \( \sum_{i=1}^{n} c_{i}T^{-1}T(b_{i}) \) (linearity of T)</li>
              <li>= \( \sum_{i=1}^{n}c_{i}b_{i} \)</li>
              <li>since B is a basis, this implies that \(c_{1}=…=c_{n} = 0\) and hence that T(B) is a linearly independent set of W</li>
              <li>Since it is both spanning and linearly in dependent,</li>
              <li>T(B) is a basis of W and thus…</li>
              <li>dim V = number of vectors in B = number of vectors in T(B) = dim W</li>
            </ul>
          </li>
          <li>(&lt;=): WTS: dim V = dim W =&gt; V and W and isomorphic
            <ul>
              <li>supposed dim V = dim W = n (so there’s n vectors in the basis)</li>
              <li>and let
                <ul>
                  <li>\( E = {\ e_{1},..,e_{n} }\ \subset V\)</li>
                  <li>\( F = {\ f_{1},..,f_{n} }\ \subset W\) be bases</li>
                </ul>
              </li>
              <li>Then by the previous claim(what is it!) the unique linear map \(T: V ]mapsto W\) such that T(E) = F (sends one basis to another) is an isomorphism. nice.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><em>notation</em>: two isomorphic spaces over the same field:= \(V \simeq W\)</li>
  <li><em>caution</em>: the previous claim just says if dim V = dim W there exists an isomorphism between them. It is not the case that any linear map between the two spaces is an isomorphism.
    <ul>
      <li>ex: \(0 : v \mapsto W\) is a linear map between the two that is not an isomorphism. Not bijective when dim V is greater than 0.</li>
      <li>the size of the Kernel helps tell if a mapping is an iso.</li>
    </ul>
  </li>
  <li><strong>Kernel</strong> (definition) : the subspace of the domain of a linear mapping \(T : v \mapsto W\) between two vector spaces
    <ul>
      <li>ker(T) = \( \{ v \in V s.t. T(v) = 0 \} \)</li>
    </ul>
  </li>
  <li><strong>Theorem</strong> ( equal dimensions and kernel has zero vector implies iso ):
    <ul>
      <li>given a linear map T between V and W, F vector spaces, that have the same finite dimension,</li>
      <li>T is an iso iff &lt;=&gt; Ker(T) = {\( \vec{0} \)}</li>
      <li><em>proof</em>:
        <ul>
          <li>S1: ( =&gt; ) T is an isomorphism / bijective, so from the “onto” criteria we have
            <ul>
              <li>there exists (for sure) a vector v in V that maps to 0 in W. This is because W needs to have the 0 vector to be a vector space.</li>
              <li>there exists \(v \in V\) s.t. \(T(v) = \vec{0} \in W\)</li>
            </ul>
          </li>
          <li>S2: But T is linear, so \(T(0) = T(0v) = 0 T(v) = 0\)</li>
          <li>S3: This shows that \(\vec{0} \in V\) is the uniqe vector in V that is mapped to 0 in W by T.
            <ul>
              <li>This proves that \(Ker(T) = 0\)</li>
              <li>-(QUESTION: does this mean that the solution set has only the trivial solutin in an isomorphism?) - the id matrix is an iso, and when set = 0 its only solution is 0, think about this, so it seems to be the case, at least for the id matrix</li>
            </ul>
          </li>
          <li>S1: (&lt;=) “kernel only contains 0 implies T is an iso” (harder!)
            <ul>
              <li>suppose \(Ker(T) = \{\vec{0}\}\)</li>
              <li>Let \(\{ e_{1},…,e_{n} \}\) be a basis of V</li>
              <li>S2: so the vectors \(\{ f_{1} = T(e_{1} … f_{n} = T(e_{n})) \} \in W\) are linearly independent. (how do we know this?)
                <ul>
                  <li>well, if we have scalars \(a_{n} \in \mathbb{F}\) s.t. \(a_{1}f_{1},…,a_{n}f_{n} = \vec{0} \in W\) then we get</li>
                  <li>\(\vec{0} = \sum_{i=1}^{n}a_{i}f_{i}\)</li>
                  <li>\(= \sum_{i=1}^{n}a_{i}T(e_{i}) \)</li>
                  <li>= \( \sum_{i=1}^{n}T(a_{i}e_{i})\) (linearity of T)</li>
                  <li>= \( T(\sum_{a=1}^{n}a_{i}e_{i}) \) = \(\vec{0}\), (linearity of T)</li>
                </ul>
              </li>
              <li>S3: but since we have that \(Ker(T) = \{\vec{0}\}\),
                <ul>
                  <li>\(Ker(T) = \{ \sum_{a=1}^{n}a_{i}e_{i} = \vec{0}\}\), and kernel is a subspace of the domain T, V, so \(\sum_{a=1}^{n}a_{i}e_{i} \in V \)</li>
                </ul>
              </li>
              <li>S4: since {e}’s were a basis of V, we have \(a_{2} = …= a_{n} = 0\)</li>
              <li>S5: we have now shows that S2 is true.
                <ul>
                  <li>This face, that the {f} are linearly independent + dim V = dim W = n (bases of V and W have the same number of vectors) shows that \(f_{1},…,f_{n}\) is a basis of W.</li>
                </ul>
              </li>
              <li>S6: Since T sends the basis of V to the basis of W, it is an isomorphism.</li>
              <li>QED</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Image of T</strong>(definition): For a linear mapping \(T: V \mapsto W\),
    <ul>
      <li>im(T) = \( \{w \in W : \exists v \in V s.t. T(v) = w\} \)</li>
      <li>EXERCISE: check that im(T) is always a subspace of W.</li>
    </ul>
  </li>
  <li><strong>Theorem</strong>: V,W, F-vector spaces with equal finite dimension. dim V = dim W &lt; \(\infty\). Then a linear map \(T: V \mapsto W\) is an iso &lt;=&gt; (iff) im(T) = W</li>
  <li>QUESTION: is this any different than the def of iso? being onto =&gt; range(T) = W? oh i see this only works for the first direction.
    <ul>
      <li><em>proof</em> : ( =&gt; )If T is an isomorphism, then
        <ul>
          <li>T is bijective (in particular onto/surjective:). Thus image(T) = W.</li>
        </ul>
      </li>
      <li>(&lt;=) “if im(T) = W then T is an isomorphism.”</li>
      <li>Suppose image(T) = W,</li>
      <li>let \(\{ f_{1},…,f_{n}\}\) is a basis of W.</li>
      <li>since im(T) = W we can find vectors \(e_{1},..,e_{n} \in V\) such that \(T(e_{i}= f_{i})\) , \(i = 1,…,n\)</li>
      <li>Then the vectors \(\{e_{1},…,e_{n}\}\) are linerly independent in V. how so? well :
        <ul>
          <li>suppose we have scalars \(a_{1},…,a_{n} \in K\)</li>
          <li>so that \( a_{1}e_{1} + .. + a_{n}e_{n} = \vec{0} \)</li>
          <li>then, \( T(\sum_{i=1}^{n} a_{i}e_{i}) = T(\vec{0})\)</li>
          <li>\( \sum_{i=1}^{n} a_{i}T(e_{i}) = T(\vec{0})\)</li>
          <li>\( \sum_{i=1}^{n} a_{i}f_{i} = T(\vec{0})\)</li>
          <li>since \( \{ f_{1},…,f_{n} \} \) is a basis in V, then we can conclude that the scalars \( a_{1}=…=a_{n} = 0\)</li>
          <li>thus, \( \{e_{1},…,e_{n}\} \) is linearly independent.</li>
        </ul>
      </li>
      <li>Since we have dim V = dim W = n, this implies that \( \{ e_{1},…,e_{n} \} \) is a basis of V</li>
      <li>and because we have a linear mapping T that sends a basis to another it is an isomorphism.</li>
      <li>QED</li>
    </ul>
  </li>
  <li>
    <p>The argument we used to prove the previous two theorems can be refined to show that Ker(T) and im(T) always have complementary dimensions.</p>
  </li>
  <li><strong>Theorem</strong>: (Rank-Nullity?) Let V,W be F-vector spaces. \( T:V \mapsto W\) is a linear map. Dimension V &lt; infinity (it’s finite). <em>note that this only talks about the dimentison of the domain</em> Then:
    <ul>
      <li>(a): dim Ker(T), dim Im(T) finite.</li>
      <li>(b): dim Ker(T), dim Im(T) = dim V</li>
    </ul>
  </li>
  <li><em>proof</em>: for part (a), note that \(Ker(T) \subset V\) is a subspace of V and by the monotonicity of dimension of spaces we get that
    <ul>
      <li>\( dim(Ker(T) \leq dim(V) \leq \infty)\) (both finite, dim ker is less)</li>
    </ul>
  </li>
  <li>Also if \( \{ e_{1},…,e_{n} \} \) is a basis of V
    <ul>
      <li>then V = span\(\{e_{i}\}\)</li>
    </ul>
  </li>
  <li>so then the image of T, which is all of T applied to every vector in V, is the span of T applied to every basis vector, by linearity of T:
    <ul>
      <li>\(Im(T) = T(V) = span(T(e_{1}),…,T(e_{n})) \)</li>
    </ul>
  </li>
  <li>from this fact it follows that \(im(T)\) is spanned by finitely many vectors, and so the dim \(im(T) \leq \infty\)</li>
  <li>For part (b) :
    <ul>
      <li>choose a basis of the Ker(T), \(\{ e_{1},…,e_{k} \}\).</li>
      <li>now choose a completion to a basis of V to a basis of V : \( \{ e_{1},…,e_{k},e_{k+1},…,e_{n} \} \)</li>
      <li>QUESTION: get an explicit example of doing this!</li>
      <li>Then we get that \(T(e_{1}),…,T(e_{n})\) span im(T)! QUESTION: why? from part a!</li>
      <li>But \(T(e_{1}) = …= T(e_{k}) = 0\), from the basis of Ker(T)</li>
      <li>so we get that really, \(T(e_{k+1}), …, T(e_{n})\) will span im(T)</li>
      <li>but the vectors \(T(e_{k+1}), … , T(e_{n})\) will be linearly independent in \(im(T) \subset W\) (<em>how so</em>)? :
        <ul>
          <li>well, if we have \(a_{k+1},…,a_{n} \in \mathbb{F}\) s.t.</li>
          <li>\(\sum_{i=k+1}^{n} a_{i}T(e_{i}) = \vec{0}\)</li>
          <li>then \( T(\sum_{i = k+1}^{n}a_{i}e_{i}) = \vec{0}\) (by linearity of T)</li>
          <li>and so \( \sum_{i = k+1}^{n}a_{i}e_{i} \in Ker(T) \)
            <ul>
              <li>because its the argument in T(0) = 0, do you see it?</li>
            </ul>
          </li>
          <li>so \(\sum_{i = k+1}^{n}a_{i}e_{i} = \sum _{j=1}^{k}b_{j}e_{j}\)</li>
          <li>since\(\{e_{1},…,e_{n}\}\) is a basis of V it follows that
            <ul>
              <li>all coefficients must be equal to zero, \(a_{k+1},…,a_{n} = 0\), (which implies linear independence?)</li>
            </ul>
          </li>
          <li>Thus \(T(e_{k+1}),…,T(e_{n})\) is a basis of im(T).</li>
          <li>This shows that dim im(T) = \(n-k\) = dim V - dim ker(T).</li>
          <li>QED</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="dual-spaces-and-isomorphism">Dual Spaces and Isomorphism</h1>

<p>We can use the criteria for isomorphic spaces to uncover more truths about the relationship between \(V \) and \(V^{v}\)</p>

<ul>
  <li><strong>Dimension of \(V^{v}\) and \(V\) Claim</strong>:
    <ul>
      <li>if V is a finite dimensional vector space over F,</li>
      <li>then (=&gt;) \(dimV^{v} = dim V\)
        <ul>
          <li><strong><em>proof</em></strong>: Let \(E = \{e_{1},…,e_{n}\} \) be a basis of V.</li>
          <li>Then for all \(i\) consider this unique linear operator / function:
            <ul>
              <li>\({e_{i}}^{v}: V \mapsto \mathbb{F}\) (from the vector space to the field) s.t.</li>
              <li>\({e_{i}}^{v}{e_{j}}\) = 
  \(\begin{cases} 
  0 &amp; j \neq 1 \\\
  1 &amp; j = i 
  \end{cases}\)</li>
            </ul>
          </li>
          <li>we claim that the functions: \(e_{1}^{v},…,e_{n}^{v} \in V^{v}\) form a basis of the dual space \(V^{v}\) of \(V\) <em>how so you ask!</em></li>
          <li>well, let \(f \in V^{v}\) be any element in the dual space, so it is a linear functional that sends vectors in v to the field.</li>
          <li>the function \(f: V \mapsto \mathbb{F}\) is unqiely determined by its values on the spanning set E of V.
            <ul>
              <li>QUESTION: so the lemma works for spanning sets, not just bases?????</li>
            </ul>
          </li>
          <li>these values are: \( f(e_{1}),…,f(e_{n}) \in \mathbb{F} \). (recall they send vectors in v, in this case e, to a scalar in the field)</li>
          <li>consider the linear function:
            <ul>
              <li>g = \( f(e_{1})e_{1}^{v},…,f(e_{n})e_{n}^{v}\)</li>
              <li>\(g(e_{j}) = f(e_{1})e_{1}^{v}(e_{j}) + … + f(e_{n})e_{n}^{v}(e_{j})\) = \(f(e_{j})\), because it becomes \( f(1*e_{j})\) for all j.</li>
              <li>Thus, \(f : V \mapsto F\) and \(g : V \mapsto F\), take the same values on the basis \(e_{1},…,e_{n}\)</li>
              <li>since linear transformations are characterized by what they do to a generating set, then \(f = g\) (remeber E is a spanning set of V)</li>
            </ul>
          </li>
          <li>This shows that \(V^{v} =\) span\( (e_{1}^{v}, …, e_{n}^{v}) \)</li>
          <li>Next, let scalars \(c_{1},…,c_{n}\) are such that \(c_{1}e_{1}^{v},…,c_{n}e_{n}^{v} = 0 \in V^{v}\)</li>
          <li>so this \(c_{1}e_{1}^{v},…,c_{n}e_{n}^{v} : V \mapsto F\) is the zero function.</li>
          <li>Evaluate the function on \( (e_{j})\), gives \( c_{1}e_{1}^{v}(e_{j}),…,c_{n}e_{n}^{v}(e_{j}) \)</li>
          <li>but value of zero mapping on any vector just gives you zero back, so
            <ul>
              <li>\( c_{1} = … = c_{n} =0\)</li>
            </ul>
          </li>
          <li>Hence, \( \{ e_{1}^{v},…,e_{n}^{v} \}\) is linearly independent, thus they form a basis, since they are also spanning, of the dual space.</li>
          <li>Hence, \(dim V^{v} = n = dim V\)</li>
          <li>QED</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Definition Dual Basis</strong>: (not complete lol)
    <ul>
      <li>\(E^{v}= e_{1}^{v},…,e_{n}^{v} \), as defined above, is the dual basis of E (of V)</li>
    </ul>
  </li>
  <li><em>note</em>: dual spaces allow us (only when V is finite dimensional!) to define an isomorphism \(T: V \mapsto V^{v}\).
    <ul>
      <li>T is defined as the unique linear map sending the basis E of V to the dual basis \(E^{v} of V^{v}\).</li>
    </ul>
  </li>
  <li>also works in opposite direction …</li>
  <li><strong>Dual Bases Claim</strong>: V is a finite dim vector space over F, let
    <ul>
      <li>\( F = \{ f_{1},…,f_{n} \} \) be a basis of \(V^{v}\)</li>
      <li>Then there exists a unique basis \(E = \{ e_{1},…,e_{n} \}\) of V s.t.\(E^{v} = F\)</li>
      <li>(a basis for the dual space can always constructed from a basis for the space, so we can find the “origin” basis for any basis in \(V^{v}\))</li>
      <li><em>proof</em>:
        <ul>
          <li>note that each vector \( x \in V \) defines a linear function
            <ul>
              <li>\(ev_{x}: V^{v} \mapsto \mathbb{F}\) alt: \(f \mapsto f(x)\) (the evaluation map! kirillov was talking about this!)</li>
            </ul>
          </li>
          <li>This gives a map:
            <ul>
              <li>\(ev : V \mapsto V^{vv}\) alt: \( x \mapsto ev_{x} \)</li>
            </ul>
          </li>
          <li>this map is linear:
            <ul>
              <li>if \(x_{1},x_{2} \in V\), \(a_{1}, a_{2} \in F\)</li>
              <li>then for every \( f \in V^{v}\) we have:</li>
              <li>\(ev_{a_{1}x_{1} + a_{2}x_{2}}(f) =V s\mapsto F f(a_{1}x_{1}+a_{2}x_{2}) = a_{1}f(x_{1}) + a_{2}f(x_{2}) = a_{1}ev_{x1}(f) + a_{2}ev_{x2}(f)\)</li>
            </ul>
          </li>
          <li>By the previous claim on \(dim V^{vv} = dim V^{v} = n\),</li>
          <li>so \(ev: V \mapsto V^{vv}\) is a linear map between vector spaces of the same dimension.</li>
          <li><em>now a brief interruption for a lemma we need to prove to finish proving this claim</em>:</li>
        </ul>
      </li>
      <li><strong>Evaluation Map is an Isomorphism Lemma:</strong>
        <ul>
          <li>For a finite dimensional vector space V over F the map \( ev: V \mapsto V^{vv} \) is an isomorphism. (a natural one!)</li>
          <li><em>proof</em>: Let \(\{ b_{1},..,b_{n} \}\) be a basis of \(V\).
            <ul>
              <li>let \(\{b_{1}^{v},…,b_{n}^{v}\}\) be a basis \(V^{v}\).</li>
              <li>let \( \{ b_{1}^{vv},…,b_{n}^{vv} \} \subset V^{vv}\) be a basis dual to the dual basis.</li>
              <li>Compute \(ev_{b_{i}}(b_{j}^{v})\).</li>
              <li>By definition, we have</li>
              <li>\( ev_{b_{i}}(b_{j}^{v}) = b_{j}^{v}(b_{i}) =  \)
  \(\begin{cases} 
  0 &amp; j \neq 1 \\\
  1 &amp; j = i 
  \end{cases}\)</li>
              <li>Thus, we can see that \( ev_{bi} \) and \( b^{vv} \) take on the same values on the dual basis \( \{ b_{1}^{v},…,b_{n}^{v} \} \)</li>
              <li>since every linear map,function is uniquely detemined by its values on a spanning set, it follows that:
                <ul>
                  <li>\( ev_{bi} = b_{i}^{vv}\) for i =1,..,n</li>
                </ul>
              </li>
              <li>Conclusion: \(ev\) sends a basis of V toa basis of \(V^{vv}\) and so it is an isomorphism.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>back to the proof….</li>
      <li>Now let us start wuth the basis
        <ul>
          <li>\( F = \{ f_{1},…,f_{n}\} \) of \(V^{v}\)</li>
        </ul>
      </li>
      <li>Let \(F^{v} = \{ f_{1}^{v},…,f_{n}^{v} \}\) be the dual basis of \(v^{vv}\)</li>
      <li>Since we have \(ev: V \mapsto V^{vv}\) is an isomorphism (proved above)</li>
      <li>for each \(i = 1,…,n\) we have a unique vector \(e_{i} \in V\) s.t.
        <ul>
          <li>\(ev_{e_{i}} = f_{i}^{v}\) (because an iso sends a basis to a basis?)</li>
        </ul>
      </li>
      <li>But then, \(f_{i}(e_{j}) = ev_{ej}(f_{i}) = f_{j}^{v}(f_{i}) = \begin{cases} 
      0 &amp; j \neq 1 \<br />
      1 &amp; j = i 
      \end{cases} \)</li>
      <li>Now consider the collection of vectors \( \{ e_{1},…,e_{n} \} \subset V\)
        <ul>
          <li>It is a basis of V since it is the image of the basis of the double dual: \(\{ f_{1}^{v},..,f_{n}^{v} \}\) of \(V^{vv}\) under the isomorphism: \(ev^{-1}: V^{vv} \mapsto V\)</li>
        </ul>
      </li>
      <li>The formula \( f_{i}(e_{j}) = \begin{cases} 
      0 &amp; j \neq 1 \<br />
      1 &amp; j = i 
      \end{cases} \) implies that ..</li>
      <li>\(f_{1},..,f_{n}\) is the dual basis of \( e_{1},…,e_{n} \) and this proves our claim.</li>
      <li>QED</li>
    </ul>
  </li>
  <li><em>Remark</em>: If \(V\) is an n-dimensional space over F, we now know that
    <ul>
      <li>\(V \simeq V^{v}\)</li>
      <li>\(V \simeq V^{vv}\)</li>
    </ul>
  </li>
  <li>However, the first one is a isomorphism that depends on the choice of the basis, the second is a <em>canonical</em> isomorphism.</li>
  <li>To construct the first, we need to find a basis of V.</li>
  <li>To construct the second, there is a canonical isomorphism \(ev: V \mapsto V^{vv}\) which is deinfed only in terms of V and doesn’t depend on a basis.</li>
</ul>

<p>** <strong>Exercises:</strong></p>
<ul>
  <li>(Duality for Linear Maps): Let U, V, W be vector spaces over F
    <ul>
      <li>(1) Show that if \(T: V \mapsto W\) is a linear map, then the map \(T^{v}: W^{v} \mapsto V^{v}\) defined by \(T^{v}(f) = f \circ T\), for any linear function \(f: W \mapsto F\) is also linear.</li>
      <li>(2) Show that if \(S: U \mapsto V\), \(T: V \mapsto W\) are linear maps, then \( (T \circ S)^{v}: S^{v} \circ T^{v}\)</li>
      <li>(3) Suppose that V,W are finite dimensional and let
        <ul>
          <li>\( E = \{ e_{1},…,e_{n} \} \subset V \)</li>
          <li>\( F = \{ f_{1},…,f_{n} \} \subset W \)</li>
        </ul>
      </li>
      <li>be bases of V and W.</li>
      <li>Let \(S \in Mat(mxn)(F)\) be the matrix of T in the bases \(E \subset V\) and \( F \subset W \).</li>
      <li>Show that the matrix of \(T^{v}: W^{v} \mapsto V^{v}\) in the dual bases \( F^{v} \subset W^{v} \) and \( E^{v} \subset V^{v} \) is the matrix \( A^{T} \in Mat(nxm)(F) \)</li>
      <li>(4) Prove that if \(A \in Mat(nxm)(F)\) and \(B \in Mat(mxk)(F)\)
        <ul>
          <li>then \((AB)^{T} = B^{T}A^{T}\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="lectures-12-13">Lectures 12-13</h3>

<ul>
  <li>####<strong>Traces</strong>:</li>
  <li>It is useful to have numerical invariants measuring the complexity of linear maps</li>
  <li>we already have some discrete (= integer invariants)
    <ul>
      <li>for every linear map \( T: V \mapsto W \)</li>
      <li>we have two integers cpaturing information about T (transformation)
        <ul>
          <li><strong>Definition: nullity of T:</strong>
            <ul>
              <li>dim Kernel(T) = dim Nullspace(T) = dim of the solution set to \(Ax=0\)</li>
            </ul>
          </li>
          <li><strong>Definition: Nullspace(T)</strong>:
            <ul>
              <li>set of all n-dimensional column vectors such that \(Ax=0), the solution set of the homog linear system.
                <ul>
                  <li><strong><em>Theorem</em></strong>: The nullspace N(A) is a subspace of the vector space \(\mathbb{R^{n}}\)</li>
                  <li>proof: WTS: N(A) is nonempty, closed under addition, closed under scalar multiplication:</li>
                  <li>S1: the trivial solution is always in N(A)- so it’s nonempty. \(\vec{x}=\vec{0}\)</li>
                  <li>S2: WTS: \( x,y \in N(A) =&gt; x+y \in N(A)\)
                    <ul>
                      <li>Well, \( Ax = 0, Ay = 0, A(x+y) = A(x) + A(y) = 0 + 0 = 0 \)</li>
                    </ul>
                  </li>
                  <li>S3: \(c \in \mathbb{R}, x \in N(A) =&gt; cx \in N(A)\)
                    <ul>
                      <li>Well, \(A(cx)=c*A(x) = c * 0 = 0\)</li>
                    </ul>
                  </li>
                  <li>QED</li>
                </ul>
              </li>
            </ul>
          </li>
          <li><strong>Definition: rank of T</strong>:
            <ul>
              <li>dim image(T) = …QUESTION: any other defs? yes, see lec 14 for longer discussion</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>turns out that for linear operators \(T: V \mapsto V\) we also have defined invariants which are scalars of the field \(\mathbb{F}\)
    <ul>
      <li>example of invariants:</li>
      <li><strong>Definition: Trace</strong>:
        <ul>
          <li>\(tr: L(V,V) \mapsto \mathbb{F}\) is :</li>
          <li>the sum of elements on the main diagonal of a square matrix A</li>
          <li>the sum of its complex eigenvalues</li>
          <li>invariant with respect to change of basis</li>
          <li>trace with this def applies to linear operators in general</li>
          <li>is a linear mapping: \( tr(T + S) = tr(T) + tr(S)\) and \( tr(cT)= c * tr(T) \)</li>
          <li>notice inside L(V,V) (linear maps from V to V) we have a natural collection of linear operators, from each one we can get a scalar back.
            <ul>
              <li>how can we get this scalar?</li>
              <li>given any pair (f,v) where
                <ul>
                  <li>\(v \in V\) is a vector</li>
                  <li>\(f \in V^{v}\) is a linear functional in the dual space = the space of all linear functionals from V to the scalar field</li>
                </ul>
              </li>
              <li>we can construct a linear operator:
                <ul>
                  <li>\(s_{f,v}: V \mapsto V, x \mapsto f(x)v \)
                    <ul>
                      <li>QUESTION: doesnt this give me a vector back?</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>but given (f,v) we can also get a natural scalar:
                <ul>
                  <li>\( f(v)\in \mathbb{F} \)</li>
                </ul>
              </li>
              <li>with this in mind we can form and prove the existence statement:</li>
              <li><strong>Existence Lemma</strong>:
                <ul>
                  <li>Suppose V is finite dimensional vector space over \(\mathbb{F}\)</li>
                  <li>Then there exists a unique linear function:
                    <ul>
                      <li>\(tr: L(V,V) \mapsto \mathbb{F}\)</li>
                      <li>such that for all \(v \in V\) and \(f \in V^{v}\)</li>
                      <li>\(tr(s_{f,v}) = f(v)\)</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>proof of lemma:
                <ul>
                  <li>fundamental fact: every linear function (any linear transformation) is uniquely determined by what it does to a basis (by its values on a basis)</li>
                  <li>from this fact, it suffices to constrct a basis of all linear functions from V to V, \(L(V,V)\) that consists of operators of the form \(s_{f,v}\) for the chosen f’s and v’s
                    <ul>
                      <li><a href="https://math.stackexchange.com/questions/2619275/whats-a-basis-for-mathcal-lv-w">more here</a></li>
                    </ul>
                  </li>
                  <li>Let \( \mathbb{B} = { b_{1}…….b_{n} } \subset V \) be any basis of V</li>
                  <li>Let \( \mathbb{B}^{v} = {b_{1}^{v}…….b_{n}^{v} } \subset V \) be its dual basis</li>
                  <li>Then we can say that the collection of operators
                    <ul>
                      <li>\( \mathbb{S} = { s_^{v},b_{1}}…….s_^{v},b_{n}} } \) is a basis of \(L(V,V)\) the set of all linear functions from V to V
                        <ul>
                          <li>basis = spanning + linearly independent.</li>
                          <li>here, each \({b_{i}}^{v}\) is a linear functional from the dual basis, and each \(b_{i}\) is a vector from the basis of V. Each gets plugged into the linear operator s and spits out a and spits out a \( {b_{i}}^{v} * b_{i} \), which is QUESTION: a vector in V?</li>
                        </ul>
                      </li>
                      <li>proof that \( \mathbb{S} \) is a basis for L(V,V):
                        <ul>
                          <li>S1: \( T: V \mapsto V \) is a linear map.
                            <ul>
                              <li>Let \( A \in Mat_{nxn}\mathbb{F} \) be the matrix of T in the basis \( \mathbb{B} \)
                                <ul>
                                  <li>note: we can always represent a linear transformation/mapping by a matrix in its</li>
                                </ul>
                              </li>
                            </ul>
                          </li>
                          <li>S2: Then \( T =  \sum_{i,j=1}^{n} a_{ij} * s_(^{v}, b_{i}}) \)
                            <ul>
                              <li>for every \( k = 1,….,n \), we have</li>
                              <li>\( T(b_{k}) =  \sum_{i=1}^{n} a_{ik} * b_{i} \)</li>
                              <li>and we also have:</li>
                              <li>\( (\sum_{ij} a_{ij}*s_^{v}b_{i}} )(b_{k}) \)</li>
                              <li>\( =  {\sum_{ij}} a_{ij}*s_^{v}b_{i}}  (b_{k}) \)</li>
                              <li>\( =  \sum_{ij} a_{ij} {b_{j}}^{v}(b_{k})b_{i} \)</li>
                              <li>\( = \sum_{i=1}^{n} a_{ik} * b_{i} \)</li>
                            </ul>
                          </li>
                          <li>S3: Thus, \( T = \sum_{ij}a_{ij}s_^{v}, b_{i}} \)</li>
                          <li>This representation is unique since the matrix of T in the basis \( \mathbb{B} \) is uniquely determined by T and \( \mathbb{B} \) 
  *(“linear extension theorem” - a linear transformation is uniqely determined by what it does to a basis.) ?</li>
                          <li>by the characterization of linear maps (the one descirbed above?) we then have a unique linear function, trace:
                            <ul>
                              <li>\( tr: L(V,V) \mapsto \mathbb{F} \)</li>
                              <li>ENDED PG 7 REVISIT AFTER LEC 9-11</li>
                            </ul>
                          </li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="lecture-14-row-reduction">Lecture 14: Row Reduction</h3>

<p><strong>Outline:</strong></p>

<ul>
  <li>Simplifying Linear Systems</li>
  <li>Row Reduction and Echelon Forms</li>
  <li>Solving Systems with Row Reduction</li>
  <li>Corollaries</li>
  <li>####<strong>Solving a Linear System</strong></li>
  <li>using row and column operations we can convert every linear system into a system in which all variables separate
    <ul>
      <li><em>row operation</em>:</li>
      <li><em>column operation</em>:</li>
    </ul>
  </li>
  <li>\(Ax=b\) where
    <ul>
      <li>\(A \in Math(mxn)(F)\) is a given coefficient matrix.</li>
      <li>\(b \in F^{m}\) is a given vector of right hand sides.</li>
      <li>\(x \in F^{n}\) is an unknown vector.</li>
    </ul>
  </li>
  <li>we proved (QUESTION: where b?) that we can find invertible matrices
    <ul>
      <li>\( R \in Mat(mxm)(F) \)  - number of eqs</li>
      <li>\( C \in Mat(nxn)(F) \) - number of unkowns</li>
      <li>such that after performing on A the row operation corresponding to R, and then the column operation corresponding to C we get the simplest possible matrix:</li>
      <li>\( \tilde{A} =  RAC =\) <img src="delpinolisette.github.io/img/blockmat1.png" alt="block matrix" />
        <ul>
          <li>where r = rank of matrix (see discussion on rank below!!)</li>
        </ul>
      </li>
      <li>Using R and C we can simplify the system Ax=b.</li>
      <li>Left multiplying (Ax=b) by R gives an equivalient system
        <ul>
          <li>\(RAx = Rb &lt;=&gt; RACC^{-1}=Rb\)
            <ul>
              <li>so the id matrix is written in the form \(CC^{-1}\) which doesnt change anything, but we have our simple matrix RAC to work with.</li>
            </ul>
          </li>
          <li>given the notation:
            <ul>
              <li>\(\tilde{A} = RAC \in Mat(mxn)F\)</li>
              <li>\(\tilde{b} = Rb \in (F^{m})\)</li>
              <li>\(\tilde{x} = C^{-1}x \in F^{n} \) = column vector with n entries.</li>
            </ul>
          </li>
          <li>write system as \(\tilde{A}\tilde{x}=\tilde{b}\), but since we used only invertible operations to get this new system, we can answer quesetions about the old system with this one.</li>
          <li>in terms of the new variables \(\tilde{x} = (\tilde{x}_{1},…,\tilde{x}_{n})^{T}\) = \(C^{-1}x\)</li>
          <li>so it becomes : 
  \[ \tilde{x}_{1} = \tilde{b}_{1} \<br />
  … \<br />
  \tilde{x}_{r} = \tilde{b}_{r} \<br />
  0 = \tilde{b}_{r+1}\<br />
  …\<br />
  0 = \tilde{b}_{m}
  \]</li>
          <li>note that it is m equations. we got this very simple system we alluded to using the block matrix previously.</li>
          <li>So we conclude that we can get the solutions of Ax=b through \(\tilde{A}\tilde{x}=\tilde{b}\)
            <ul>
              <li><em>proof</em>:</li>
              <li>S1: The tilde system and Ax = b are both consistent iff \(\tilde{b}_{r+1}= … = \tilde{b}_{m}=0\)</li>
              <li>S2: If \( \tilde{b}_{r+1}=…=\tilde{b}_{m}=0\) then the solutions of \( \tilde{A}\tilde{x}=\tilde{b} \) are vectors of the form: \[ \tilde{b}_{1} \<br />
   … \<br />
  \tilde{b}_{r} \<br />
  \tilde{x}_{r+1} \<br />
  … \<br />
  \tilde{x}_{n}  \]</li>
              <li>with \( \tilde{x}_{r+1},…,\tilde{x}_{n} \in \mathbb{F} \) being free variables</li>
              <li>S3: If \(\tilde{b}_{r+1}= … =\tilde{b}_{m}=0\) then the solutions of \(Ax=b\) are of the form: x = 
  \[ \tilde{b}_{1} \<br />
   … \<br />
  \tilde{b}_{r} \<br />
  \tilde{x}_{r+1} \<br />
  … \<br />
  \tilde{x}_{n}  \]</li>
              <li>with \( \tilde{x}_{r+1},…,\tilde{x}_{n} \in \mathbb{F} \) being free variables.\, and the whole column vector of solutions is multiplied by \(C\). (why? because solutions \(\tilde{x} were our solutions x multiplied by C^{-1}\) !)</li>
            </ul>
          </li>
          <li>but this method is not constructive, we had to pick bases in \(F^{n}, F^{m}\) to fit with A. We had actually construct R and C by choosing these bases, adopted to map T, or matrix of map A. Row reduction algorithm solves the issue.
            <ul>
              <li>the algo simplifies the system systematically, uses only row ops, done in simple steps, and allows us to get close enough to solve the system.</li>
              <li>also they solve the systm in <em>finitely</em> many steps!</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>####<strong>Elementary Row Operations</strong>:
    <ul>
      <li>note that Ax=b is the matrix equation for our equation \(F(v) = b\), once we choose a basis in the vector space V.</li>
      <li>If A is (mxn), the augmented matrix is (m x n+1).</li>
      <li>Elemtary Row Operations are just left multiplication by a specific elementary matrix. We called it R above.
        <ul>
          <li>they just replace rows in the matrix with linear combinations of rows, in an invertible way (you can always work backwards).</li>
          <li>The important question is - why is it that these row operatiors don’t change the solution set?? Because they are all invertible, reversible.</li>
          <li>the inverses of ERO’s are just ERO’s of the same type.</li>
          <li><img src="delpinolisette.github.io/img/ERO.png" alt="ERO's and inverses:" /></li>
          <li>What are these special elementary matrices? Well, they are just obtained by doing the corresponding row operation to the identity matrix.
            <ul>
              <li>Row operation 1: switch two rows: <img src="delpinolisette.github.io/img/type1.png" alt="type 1" /></li>
              <li>Row operation 2: multiply a row by a scalar multiple: <img src="delpinolisette.github.io/img/type2.png" alt="type 2" /></li>
              <li>Row operation 2: add a scalar multiple of a row to another row: <img src="delpinolisette.github.io/img/type3.png" alt="type 3" /></li>
            </ul>
          </li>
          <li>another way of showing that a row operation does not change the solution set of a system, matrix equation style :) E is the elementary matrix representating a row operation.
            <ul>
              <li>\(Ax=b\)</li>
              <li>\(EAx = Eb\) (implies any solution of this equation is a solution of the previous, because we multiplied them all by the same matrix)</li>
              <li>\(E^{-1}EAx = E^{-1}Eb\)</li>
              <li>\(Ax = b\), QED.</li>
            </ul>
          </li>
          <li><strong>claim</strong>: the composition of row operations is also a row operation!
            <ul>
              <li><em>proof</em>: if \(p_{1},…,p_{s}: \mathbb{F}^{n} \mapsto \mathbb{F}^{n}\)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>####<strong>Rank of a Matrix</strong> :
    <ul>
      <li>recall that left multiplication by an mxn matrix \(A\) defines a linear map:
        <ul>
          <li>\(T : \mathbb{F}^{n} \mapsto \mathbb{F}^{m}\) :: \(v \mapsto Av\)</li>
        </ul>
      </li>
      <li>the rank of the matrix A is then the rank of T.
        <ul>
          <li>rank(T) = dimension of the subspace \( im(T) \subset \mathbb{F}^{m}\) of the codomain.</li>
        </ul>
      </li>
      <li><strong>rank (A)</strong> (def): rank (A) = rank(T) = dim im(T) = dim (column space of A) = maximal number of linearly independent vectors in col space A = max number of linearly independent columns of A.</li>
      <li><strong>im(T)</strong> (def): im(T) = \(y \in F^{m} : y = T(v) = Av\) for some \(v \in F^{n}\) = span(columns of A)
        <ul>
          <li>(we show the last part now), that im(T) = span of columns of A.</li>
          <li>now note that:</li>
          <li>\(F^{n}\), the domain of the linear map, which is a coordinate n space, is spanned by the vectors of the cardinal basis \(e_{1},…,e_{n}\) and therefore \(im(T)\) is spanned by the vectors \(Ae_{1},…,Ae_{n} \in F^{m}\), which as we now discuss are just the columns of A
            <ul>
              <li>we know that image of a linear map is gneerated by the image of the generators of the source space (QUESTION: - when did we discuss this?)</li>
              <li>recall that each \(e_{i}\) has a 1 in the ith spot, 0 everywhere else, so by definition of matrix multiplication (row by column),
                <ul>
                  <li>\(Ae_{i}\) = i-th column of A.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Thus, im(T) = span(columns of A)</li>
        </ul>
      </li>
      <li>To be continued</li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="chapter-1">Chapter 1</h3>

<hr />

<h3 id="chapter-2">Chapter 2</h3>
<h4 id="section-23--analyzing-the-pivots">Section 2.3 : Analyzing the Pivots:</h4>
<ul>
  <li>All questions about the existence of a solution + uniqueness of a solution of a system can be answered by : look at the <em>pivots</em> in RREF form of the augmented matrix of the system. (needs to be \(A^{rref}\))!</li>
  <li><strong>when is \(Ax=b\) inconsistent?</strong>
    <ul>
      <li>when it doesn’t have a solution!</li>
      <li>(1) a system is <strong>inconsistent</strong> (&lt;=&gt;) (iff)
        <ul>
          <li>there is a pivot in the last row of the augmented matrix.</li>
          <li>so it looks like (\(0 0 0 0 : 1\))
            <ul>
              <li>this implies the last equation is \(0x_{1} + … + 0x_{n} = b, b\neq 0\),</li>
              <li>=&gt; no solution.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>remarks on the coefficient matrix</strong>:
    <ul>
      <li>(1): a solution, if it exists, is unique \(\iff\) :
        <ul>
          <li>there are no free variables</li>
          <li>the echelon form of the coefficient matrix has a pivot in each <em>column</em>,</li>
          <li>(note that this says nothing about the existence, only uniqueness of solution.)</li>
          <li>QUESTION: how does this connect to rank, and the fact that rank(A) = dim column space of A?</li>
        </ul>
      </li>
      <li>(2): \(Ax = b\) is consistent (has a solution) for all b \(\iff\) :
        <ul>
          <li>the echelon form of the <em>coefficient matrix</em> has a pivot in each_row_</li>
          <li>note: this property makes sense considering if there is a pivot in each row of coefficint matrix, you can’t have a pivot in the last column on the augmented matrix. =&gt; never inconsistent.</li>
        </ul>
      </li>
      <li>(3): Ax = b has a <em>unique solution</em> (and it exists..) for all b iff \(\iff\):
        <ul>
          <li>echelon form of the coefficient matrix \(A\) has a pivot in each row and each column.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>remarks on inconsistency</strong> (very cool)
    <ul>
      <li>WTS: To be continued</li>
    </ul>
  </li>
</ul>

<h4 id="section-27--fundamental-subspaces-and-rank">Section 2.7 : Fundamental Subspaces and Rank:</h4>
<ul>
  <li>four fundamental subspaces:
    <ul>
      <li>Ker(A), Ran(A), row space(A), Left nullspace(A)</li>
      <li>need to study relationships between their dimensions.</li>
    </ul>
  </li>
  <li>any linear transformation has subspaces : Ker(A), Range(A)
    <ul>
      <li><strong>Ker(A)</strong>, subset of domain \(V\) :
        <ul>
          <li>Ker(A) = Nullspace(A) := \(\{ v \in V : Av = 0 \} \subset V\)</li>
          <li>solution set of the homogenous equation Ax = 0</li>
          <li>Ker(\(A^{T}\)) = left null space.</li>
        </ul>
      </li>
      <li><strong>Ran(A)</strong>, subset of codoamin \(W\) :
        <ul>
          <li>Range(A) := \(\{w \in W : w = Av, v \in V\}\)</li>
          <li>set of all right sides \(b \in W\) for which \(Ax = b\) has a solution, “is consistent.”</li>
          <li>Range(A) = column space(A) (a basis is pivot columns’ originals)</li>
          <li>Range(\(A^{T}\)) = row space (a basis is pivot rows)</li>
        </ul>
      </li>
      <li><strong>rank (A)</strong> = dim Range(A) = dim Colspace(A)</li>
    </ul>
  </li>
  <li>Computing fundamental subspaces:
    <ul>
      <li>S1: reduce \(A\) to \(A_{e}\) (echelon form of A).</li>
      <li>S2: <em>original</em> columns of \(A\) which becomes pivot columns of \(A_{e}\) are a <em>basis</em> in Range(A).</li>
      <li>S3: pivot rows of \(A_{e}\) are a basis for rowspace(A)
        <ul>
          <li>you can also transpose the row matrix and do row operations, but easier this way.</li>
        </ul>
      </li>
      <li>S4: basis for Ker(A) = nullspace(A) comes from solving homogenous \(Ax = 0\).
        <ul>
          <li>Consider \(A\) and \(A_{e}\):</li>
          <li>A = \(  \begin{bmatrix}
     1 &amp; 1 &amp; 2 &amp; 2 &amp; 1\<br />
     2 &amp; 2 &amp; 1 &amp; 1 &amp; 1 \<br />
     3 &amp; 3 &amp; 3 &amp; 3 &amp; 2 \<br />
     1 &amp; 1 &amp; -1 &amp; -1 &amp; 0
     \end{bmatrix}\), \(A_{e}\) = \( \begin{bmatrix}
                                         1 &amp; 1 &amp; 2 &amp; 2 &amp; 1 \<br />
                                         0 &amp; 0 &amp; -3 &amp; -3 &amp; -1 \<br />
                                         0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
                                         0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
                                         \end{bmatrix}\)</li>
          <li>since columns \(1,3\) becomes pivot columns in \(A_{e}\), then the columns 1 and 3 of \(A\) are a basis for colspace(A) = Ran(A). (notice that the dimension of the colspace/ran(A) is 2.)</li>
          <li>since rows \(1,2\) are pivot rows of \(A_{e}\), they form a basis for rowspace(A), in their REF. (notice dim rowspace(A) = 2).</li>
          <li>now, solving in REF, we get the solution set of A, which can be written in vector form \[ \begin{bmatrix}
                          -t-(1/3)r \<br />
                          t \<br />
                          -s-(1/3)r \<br />
                          s \<br />
                          r
                          \end{bmatrix} \]</li>
          <li>or as \(\begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}\) t + \(\begin{bmatrix} 0 \\ 0 \\ -1 \\ 1 \\ 0 \end{bmatrix}\)s + \(\begin{bmatrix} -(1/3) \\ 0 \\ (1/3) \\ 0 \\ 1 \end{bmatrix}\)r.
            <ul>
              <li>these are then the three basis vectors, and the dimension of the kernel is 3.</li>
              <li>note that there is no shortcut for computing Ker(\(A^{T}\)), you need to solve \(A^{T}x = 0\) by hand.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Explanation for computing bases of fundamnetal subspaces</strong>: why do these methods work ?
    <ul>
      <li></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="chapter-3">Chapter 3</h2>

<hr />

<h2 id="chapter-4">Chapter 4</h2>

<ul>
  <li>Spectral Theory</li>
  <li>####notes for hw 10 - on spectral theory
    <ol>
      <li>Eigenvectors are those vectors parallel to vector x,</li>
    </ol>
    <ul>
      <li>just means that Ax is some multiple of x , Ax = \(\lamba\)x</li>
      <li>lambda can be anything, negative, or even zero, just as long as it makes lamba x parallel to x.</li>
      <li>eigenvectors are the vectors that dont get knowcked off their span in the space after the transformation is applied - their egeinvalues is how much they are stretcehd by the transformation (matrix multiplication)
        <ul>
          <li>so Av, where v is eigenvector = lambda v, just says multiplying your special eigencector by a matrix A is the same effect as scalar multiplication on that matrix, it’s just scaled</li>
          <li>scaling by \(\lambda\) is the same as having an Id matrix with only \(\lambda\) down the diagonal, this is lambda times the Id matrix times the eigenvector</li>
          <li>important for finding shortcuts to rotation matrices</li>
          <li>another way to say this is that the vector x is in the ker(A-lambda I)
            <ul>
              <li>now youve sset it up as a homogenous linear system to solve for the kernel, you only have two options: infintely many sols or the trivial sol, you’re not interested in the trivial sol because you want to form an eigenbasis, so interested in when it has infinitely many sols</li>
              <li>RECALL: unique solution exists IFF full rank IFF M is invertible IFF det M nonzero
                <ul>
                  <li>so, there are infinitely many solutions IFF det M is zero</li>
                  <li>thats why we solve for zero when taking eigenvalues, M = A-labdaI</li>
                </ul>
              </li>
              <li>this determinant of M is the characteristic polynomial, its roots are eigenvalues of A</li>
              <li>to find the eigenbectors, plug into ker M. why?</li>
            </ul>
          </li>
          <li>the nullspace M + 0 vector = eigenspace</li>
          <li>the set of all eigenvalies of an operator or matrix A is called the spectrum of A, and denoted o(A)</li>
          <li>how to find the eigenvalues for an abstract operator A?
            <ul>
              <li>well just take an arbritary basis and compute eigenbalies of the abstract operator in the matrix of the operator wrt the basis,</li>
              <li>this works because it doesnt depend on choice of basis - see similar matrices explanation : characteristic polynomails of similar matrices coincide</li>
            </ul>
          </li>
          <li>algebraic multiplicity of a root lambda - # of times it shows up in sol</li>
          <li>geometric multiplicity of the eigenvalue lambda = dimension of the eigenspace Ker(A-LI)
            <ul>
              <li>dimension of eigenspace / geo multiplicity of eigenvalue cannot exceed its algebraic multiplicity</li>
            </ul>
          </li>
          <li><strong>trace and det (new interpretations</strong>: , L= \(\lamda\)
            <ul>
              <li>trace(A): L1 + L2 + … + Ln</li>
              <li>det(A): L1L2…Ln</li>
            </ul>
          </li>
          <li><strong>shortcut: eigenvalues of traingular matrices</strong>:
            <ul>
              <li>eigenvalues of a triangular matrix are exactly the diagonal entries of the original matrix A</li>
              <li>=&gt; so entries of a diagonal matrix are its diagonal entries</li>
            </ul>
          </li>
          <li><strong>similar matrices have the same eigenvalues, different vectors</strong></li>
          <li>of course, if matrices have the same eigenvals and same eigenv’s theyre the same matrix</li>
          <li>proof: Similar matrix def: A, B similar if \(B = M^{-1}AM\)
            <ul>
              <li>\(Bx = M^{-1}AMx = \lambda x = MM^{-1}AMx = M\lambda x = A(Mx) = \lambda (Mx) \)
                <ul>
                  <li>notice they have the same eigenvalues but different eigenvectors! \(i.e: Mx vs x\)</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>smilar matrices represent same linear transformation in different bases.</li>
          <li><strong>a matrix relating two similar matrices (two matrices with same eigenval</strong></li>
          <li>is the eigenvector matrix</li>
          <li><strong>\(A^{T}\) is similar to \(A\)</strong>
  ** <strong>Section 4.2:</strong></li>
        </ul>
      </li>
      <li>task of diagonalization: find a basis in which the matrix is diagonal</li>
      <li>not all matrices/operators are diagonalizable
        <ul>
          <li>we only care because if matrices/operators are diagonalizable then their powers or functions on them are easy to compute. applications explained here: <a href="https://www.math.upenn.edu/~moose/240S2013/slides7-25.pdf">diagonalizable applications</a></li>
          <li>necessary and sufficient condition for diagonalization:</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="chapter-9">Chapter 9</h2>
<ul>
  <li><strong>Cayley Hamilton Theorem</strong>: (how to use it)
    <ul>
      <li>plugging the original matrix A into the characteristic polynomial (def of A-lambdaxI determinant) gives you the zero matrix</li>
      <li>every matrix “solves” / is a root of its characteristic polynomial. cool.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="chapter-8-dual-spaces-and-tensors">Chapter 8: Dual Spaces and Tensors</h2>
<ul>
  <li>all spaces here are finite dimensional</li>
  <li>might wanna refer to <a href="#dual-spaces-and-isomorphisms">Dual Spaces and Isomorphisms</a></li>
</ul>

<h3 id="section-81--dual-spaces">Section 8.1 : Dual Spaces</h3>
<ul>
  <li><strong>Definition: Linear Functional</strong> :
    <ul>
      <li>a special type of linear transformation on a vector space \(V\) that sends a vector from the vector space to a scalar in the field, \(L: V \mapsto \mathbb{F}\)</li>
    </ul>
  </li>
  <li>Examples?
    <ul>
      <li>a velocity vector in a given direction (physical object) getting mapped to its magnitude (a scalar) - a linear measurement</li>
      <li>a force vector getting mapped to its magnitude</li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="extra-notesdefs-to-categorize-later">Extra notes/defs to categorize later</h3>
<p><strong>Dual Spaces and Dual Basis</strong></p>
<ul>
  <li>The dual space of V is the set of all linear functionals from V to \(\mathbb{F}\( , so : \(V^{v} = T: V \mapsto \mathbb{F} \(
    <ul>
      <li>all such elements of dual space are linear functionals</li>
    </ul>
  </li>
  <li>if dim(V) &lt; \( \infty\) =&gt; \( V \) and \( V^{v} \) are isomorphic
    <ul>
      <li>to show this is true, show that they have the same dimension</li>
      <li>another way to show the isomorphism is to use the dual basis
        <ul>
          <li>linear extension theorem: says if you know what T does on basis vectos, you know what T does on every vector:
            <ul>
              <li>Let \(\mathbb{B} = { v_{1}….v_{n} }\(</li>
              <li>enough to know what \(f(v_{1}),…..,f(v_{n})\( is</li>
              <li>usually hard to graphically represent linear transformations, but since the codoamin of all such linear functions are scalars, \(f: V \mapsto \mathbb{F}\(, you can draw a graph of your linear functionals (where your inputs end up.)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Isomorphism</strong></p>
<ul>
  <li>mapppings that are injective and surjective (1:1 and onto)</li>
  <li><a href="https://www.youtube.com/watch?time_continue=6&amp;v=-Iww3p1-_bA&amp;feature=emb_logo">watch this</a></li>
  <li><strong>How to check if a mapping is an isomorphism between two vector spaces:</strong>
    <ul>
      <li>a linear transformation \(T: V \mapsto W\) is one-to-one if:
        <ul>
          <li>\(T\) maps distinct vectors in V to distinct vectors in W</li>
          <li>check if two distinct vectors in the domain give you two different vectors in the codomain when you apply the linear transformation to it.</li>
        </ul>
      </li>
      <li>it is onto if:
        <ul>
          <li>range(T) = W</li>
          <li>so take any vector in W and you can find some vector in V that maps to it</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="determinants">Determinants</h2>
<p>For an n×n matrix, each of the following is equivalent to the condition of the matrix having determinant 0</p>

<ul>
  <li>
    <p>The columns of the matrix are dependent vectors in Rn</p>
  </li>
  <li>
    <p>The rows of the matrix are dependent vectors in Rn</p>
  </li>
  <li>
    <p>The matrix is not invertible.</p>
  </li>
  <li>
    <p>The volume of the parallelepiped determined by the column vectors of the matrix is 0</p>
  </li>
  <li>
    <p>The volume of the parallelepiped determined by the row vectors of the matrix is 0</p>
  </li>
  <li>
    <p>The system of homogenous linear equations represented by the matrix has a non-trivial solution.</p>
  </li>
  <li>
    <p>The determinant of the linear transformation determined by the matrix is 0</p>
  </li>
</ul>

<p>The free coefficient in the characteristic polynomial of the matrix is 0
Depending on the definition of the determinant you saw, proving each equivalence can be more or less hard.</p>

<h3 id="exercises">exercises</h3>

<ol>
  <li>prove tr\((AB)^{-1}\) = \(B^{-1}A^{-1}\)</li>
  <li>prove inverse matrix is unique
    <ol>
      <li><a href="https://www.youtube.com/watch?v=WJfODc-3wIE">here</a></li>
    </ol>
  </li>
  <li><a href="https://www.youtube.com/watch?v=ptZy9n7JmWY">theorems in this vid</a></li>
  <li><a href="https://www.youtube.com/watch?v=qyyfb0ey-HM">exercises in this vid</a></li>
  <li><a href="https://www.youtube.com/watch?v=F2XsfzhSv2A">exercises in this vid</a></li>
  <li><a href="https://www.youtube.com/watch?v=Vmr6EAQQazA">vid</a></li>
  <li><a href="https://www.youtube.com/watch?v=jNtiENbAcFM">vid</a></li>
  <li><a href="https://www.youtube.com/watch?v=Rcj1-E3SAhs&amp;list=PLwn184d0DusOyVm8Eh-gzRyVmf3nHJ68I&amp;index=64&amp;t=695s">intersection of subspaces is a subspace, union?</a></li>
  <li>[more info] (https://math.stackexchange.com/questions/1722921/injective-or-surjective-and-same-dimension-implies-vector-space-isomorphism)</li>
  <li>[do this] (https://math.stackexchange.com/questions/2521291/proof-that-f-is-an-isomorphism-if-and-only-if-f-carries-a-basis-to-a-basis)</li>
  <li><a href="https://math.stackexchange.com/questions/170481/motivation-to-understand-double-dual-space">look at this!</a></li>
  <li><a href="https://math.stackexchange.com/questions/292353/v-is-isomorphic-to-v-ast-ast-the-double-dual-space-of-v">exericse of double dual</a></li>
  <li><a href="https://www.youtube.com/watch?v=mVeuZzJdd1w&amp;list=PLwn184d0DusOyVm8Eh-gzRyVmf3nHJ68I&amp;index=58&amp;t=0s">eigenvales and eigenvectors</a></li>
  <li><a href="https://www.youtube.com/watch?v=KUuxdk_V7To">similar matrices</a></li>
  <li><a href="https://yutsumura.com/determine-dimensions-of-eigenspaces-from-characteristic-polynomial-of-diagonalizable-matrix/">eigenspaces</a></li>
  <li><a href="https://math.stackexchange.com/questions/2676557/prove-that-t-is-diagonalizable-if-and-only-if-the-minimal-polynomial-of-t-has-no">interesting</a></li>
</ol>
:ET