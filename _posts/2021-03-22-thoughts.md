---
title: "Integration Thoughts"
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

It's a bit painful to admit but I think I can only work on **very** new or difficult technical concepts a few hours a day. In some ways, this constraint is great because it forces me to get my butt in a chair first thing in the morning, before anything else, and do something difficult. 

On another note, it's always incredible to see how some simple mathematical manipulation can make your work easier when building models in Machine Learning. For example, the Maximum Likelihood Estimate reduces to a lovely sum in the case of two classifiers. More discussion coming soon, but I've been reading about this across several papers and technical blogs:

- Elements of Statistical Inference (my Bible)
- [Logistic Regression Handout from Stanford CS 109](https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/220-logistic-regression.pdf)
- [CMU Stat Dept handout](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf)
- [CZep's Paper](https://czep.net/stat/mlelr.pdf)
- [Rutgers Slides on Logit](https://www.stat.rutgers.edu/home/pingli/papers/Logit.pdf)


I hope these links ride forever on the endless highway of the internet, shiny and chrome (please don't be broken in a year)