<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="layout" content="default"><style>body {
  max-width: 980px;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 45px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headerlink {
  font: normal 400 16px fontawesome-mini;
  vertical-align: middle;
  margin-left: -16px;
  float: left;
  display: inline-block;
  text-decoration: none;
  opacity: 0;
  color: #333;
}

.markdown-body .headerlink:focus {
  outline: none;
}

.markdown-body h1 .headerlink {
  margin-top: 0.8rem;
}

.markdown-body h2 .headerlink,
.markdown-body h3 .headerlink {
  margin-top: 0.6rem;
}

.markdown-body h4 .headerlink {
  margin-top: 0.2rem;
}

.markdown-body h5 .headerlink,
.markdown-body h6 .headerlink {
  margin-top: 0;
}

.markdown-body .headerlink:hover,
.markdown-body h1:hover .headerlink,
.markdown-body h2:hover .headerlink,
.markdown-body h3:hover .headerlink,
.markdown-body h4:hover .headerlink,
.markdown-body h5:hover .headerlink,
.markdown-body h6:hover .headerlink {
  opacity: 1;
  text-decoration: none;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .codehilite pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* MultiMarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px fontawesome-mini;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\e157';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><style>/*GitHub*/
.codehilite {background-color:#fff;color:#333333;}
.codehilite .hll {background-color:#ffffcc;}
.codehilite .c{color:#999988;font-style:italic}
.codehilite .err{color:#a61717;background-color:#e3d2d2}
.codehilite .k{font-weight:bold}
.codehilite .o{font-weight:bold}
.codehilite .cm{color:#999988;font-style:italic}
.codehilite .cp{color:#999999;font-weight:bold}
.codehilite .c1{color:#999988;font-style:italic}
.codehilite .cs{color:#999999;font-weight:bold;font-style:italic}
.codehilite .gd{color:#000000;background-color:#ffdddd}
.codehilite .ge{font-style:italic}
.codehilite .gr{color:#aa0000}
.codehilite .gh{color:#999999}
.codehilite .gi{color:#000000;background-color:#ddffdd}
.codehilite .go{color:#888888}
.codehilite .gp{color:#555555}
.codehilite .gs{font-weight:bold}
.codehilite .gu{color:#800080;font-weight:bold}
.codehilite .gt{color:#aa0000}
.codehilite .kc{font-weight:bold}
.codehilite .kd{font-weight:bold}
.codehilite .kn{font-weight:bold}
.codehilite .kp{font-weight:bold}
.codehilite .kr{font-weight:bold}
.codehilite .kt{color:#445588;font-weight:bold}
.codehilite .m{color:#009999}
.codehilite .s{color:#dd1144}
.codehilite .n{color:#333333}
.codehilite .na{color:teal}
.codehilite .nb{color:#0086b3}
.codehilite .nc{color:#445588;font-weight:bold}
.codehilite .no{color:teal}
.codehilite .ni{color:purple}
.codehilite .ne{color:#990000;font-weight:bold}
.codehilite .nf{color:#990000;font-weight:bold}
.codehilite .nn{color:#555555}
.codehilite .nt{color:navy}
.codehilite .nv{color:teal}
.codehilite .ow{font-weight:bold}
.codehilite .w{color:#bbbbbb}
.codehilite .mf{color:#009999}
.codehilite .mh{color:#009999}
.codehilite .mi{color:#009999}
.codehilite .mo{color:#009999}
.codehilite .sb{color:#dd1144}
.codehilite .sc{color:#dd1144}
.codehilite .sd{color:#dd1144}
.codehilite .s2{color:#dd1144}
.codehilite .se{color:#dd1144}
.codehilite .sh{color:#dd1144}
.codehilite .si{color:#dd1144}
.codehilite .sx{color:#dd1144}
.codehilite .sr{color:#009926}
.codehilite .s1{color:#dd1144}
.codehilite .ss{color:#990073}
.codehilite .bp{color:#999999}
.codehilite .vc{color:teal}
.codehilite .vg{color:teal}
.codehilite .vi{color:teal}
.codehilite .il{color:#009999}
.codehilite .gc{color:#999;background-color:#EAF2F5}
</style><title>line_alg</title></head><body><article class="markdown-body"><script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h4 id="table-of-contents">Table of Contents:<a class="headerlink" href="#table-of-contents" title="Permanent link"></a></h4>
<ul>
<li><a href="#lectures-9-11">Lectures 9-11</a></li>
<li><a href="#lectures-12-13">Lectures 12-13</a></li>
<li><a href="#lecture-14-row-reduction">Lecture 14</a></li>
<li><a href="#chapter-1">Chapter 1</a></li>
<li><a href="#chapter-2">Chapter 2</a></li>
<li><a href="#chapter-8-dual-spaces-and-tensors">Chapter 8</a></li>
</ul>
<h3 id="lectures-9-11">Lectures 9-11<a class="headerlink" href="#lectures-9-11" title="Permanent link"></a></h3>
<p><strong>Composition of Linear Maps</strong>   </p>
<ul>
<li>Recall that if X,Y,Z are all sets, and \( f: X \mapsto Y \) and \( g: Y \mapsto Z \) <em>if we have a mapping f from X to Y and a mapping g from Y to Z</em></li>
<li>we can form a composite map:<ul>
<li>\( g \circ f : X \mapsto Z\) that is defined by \( g \circ f = g(f(x)) \) for all \( x \in X \)</li>
</ul>
</li>
<li>we already used the notion of composition to define invertible/bijective maps of sets. </li>
<li>composition also preserves the property of maps being linear!! <em>verycool</em> we state it formally and prove it:</li>
<li><strong><em>Claim</em></strong>: The composition of two linear maps is also linear. Let U, V, W be \( \mathbb{F} \) vector spaces. <ul>
<li>Let these be linear maps:</li>
<li>\( T: U \mapsto V \) and \( S: V \mapsto W \)</li>
<li>the compositon of these is also linear:</li>
<li>\( S \circ T : U \mapsto W \)</li>
</ul>
</li>
<li><em>proof</em>: Let \( x,y \in V \), \( c \in K \). Then:<ul>
<li>\( S \circ T(x+y) = S(T(x+y)) = S(T(x) + T(y)) = S(T(y)) + S(T(y)) = S \circ T(x) + S \circ T(y) \)</li>
<li>also, \( S \circ T(cx) = S(T(cx)) = S(cT(x)) = cS(T(x)) = cS \circ T(x) \)</li>
<li>QED</li>
</ul>
</li>
<li>Thus the operation of compositions gives yet another way of constructing new linear maps out of known ones.   </li>
<li>Under our dictionary that uses bases to convert linear maps to matrices and matrices back to linear maps (QUESTION: lets see an example)<ul>
<li>=&gt; the natural operations on linear maps that produce new linear maps just become the natural operations of matrices (that produce new matrices?)</li>
<li><strong><em>Theorem</em></strong>: Let U, V, W, be \( \mathbb{F} \) -vector spaces with bases:<ul>
<li>\( \mathbb{E} = { e_{1},.......,e_{n} } \subset U \)</li>
<li>\( \mathbb{F} = { f_{1},.......,f_{n} } \subset V \)</li>
<li>\( \mathbb{G} = { g_{1},.......,g_{n} } \subset W \)</li>
</ul>
</li>
<li>(1) : If \( T_{1}: U \mapsto V, T_{2}: U \mapsto V  \) are linear mappings, \( c \in \mathbb{F} \)<ul>
<li>and \( A_{1}, A_{2} \in Mat_{mxn}\mathbb{F}\) are matrices of \( T_{1}, T_{2} \) in the bases \( \mathbb{E}, \mathbb{F} \)</li>
<li>then matrices of :<ul>
<li>\( T_{1} + T_{2}: U \mapsto V \) == \( A_{1} + A_{2} \)</li>
<li>\( cT_{1}: U \mapsto V \) == \( cA_{1} \)</li>
</ul>
</li>
<li>in our bases \( \mathbb{E} \) and \( \mathbb{F} \)  </li>
</ul>
</li>
<li>(2) :  (Linear mapping composition corresponds to matrix multiplcation): if \( T: U \mapsto V \) and \( S: V \mapsto W \) are linear mappings and <ul>
<li>T has an (m x n) matrix A with bases E (from U) and F (from V) [bases from domain and codomain]</li>
<li>S has a (k x m) matrix B with bases F (from V) and G (from W) [bases from domain and codomain]</li>
</ul>
</li>
<li>Then \( S \circ T \) has an (k x n) matrix B*A in the bases E (from domain U) and G (from codomain W)</li>
<li><em>proof</em>: <ul>
<li>(1) follows from the definition of the matrix of a linear map</li>
<li>(2) Consider the linear map \( f:= S \circ T: U \mapsto W \) and let C be the matrix of of that operator \( S \circ T \) in the bases E (from domain U) and G (from codomain W)<ul>
<li>every entry of C \( c_{pq} \) is given by:</li>
<li>\( c_{qp} = \) the q-th coordinate of the vector \( f_{e_{p}} \) in the basis \( \mathbb{G} = (g_{1}&hellip;g_{n}) \)</li>
<li>we can actually compute each entry \( c_{qp} = f_{e_{p}} \) directly: </li>
<li>\( f_{e_{p}} = S \circ T(e_{p})  \)</li>
<li>= \( S(T(e_{p})) \)</li>
<li>= \( S(A_{1p}f_{1} +&hellip;+ A_{mp}f_{m}) \) QUESTION: where does m come from?</li>
<li>= \( \sum_{r=1}^{m} A_{rp} (S_{f_{r}}) \)</li>
<li>= \( \sum_{r=1}^{m} A_{rp} (\sum_{q=1}^{k} B_{qr} g_{q})\)</li>
<li>= \( \sum _{1}^{m} (\sum_{1}^{m} B_{qr}A_{rp}) g_q{} \)</li>
<li>so we have \( c_{qp} = \sum_{1}^{m} B_{qr}A_{rp} \)</li>
<li>= (row q of B)(column p of A) = \( (BA)_{qp} \)   </li>
</ul>
</li>
</ul>
</li>
<li>Example 1 : (General Reflections)<ul>
<li>Let \( L \in \mathbb(R)^{2}\) be any line through the origin and let the transformaition \(s_{L} : R^{2} \mapsto R^{2}\) be the reflection across L</li>
<li>so \(s_{L}\) send a point in R^{2} to its mirrror image, where them mirror is L</li>
<li><strong><em>Claim</em></strong> : \(s_{L}\) is a linear map. </li>
<li><em>proof</em>: you can prove this directly using difficult geometric constructions, or you can use the fact that compositions of linear maps are linear:</li>
<li>S1: show that the reflection across the x-axis is linear:<ul>
<li>\(s = s_{x-axis} : \mathbb{R^{2} \mapsto \mathbb{R^{2}}}\)</li>
<li>\s(x, y)^{t} = (x -y)^{t}\) for all vectors x,y</li>
<li><img alt="reflection over x axis" src="/C:/Users/Lisette%20del%20Pino/Documents/github_blog_site/delpinolisette.github.io/delpinolisette.github.io/img/reflectx.png" /></li>
<li>reflection over x axis matrix == </li>
<li>A = \(
            \begin{bmatrix}
            1 &amp; 0  \\
            0 &amp; -1  \\
            \end{bmatrix}<br />
            \)</li>
</ul>
</li>
<li>S2: notice that reflection across L can be viewed as a composition of three linear maps:<ul>
<li>
<ol>
<li>rotate the plane so that L becomes the x-axis</li>
</ol>
</li>
<li>
<ol start="2">
<li>reflect the plane across the x axis </li>
</ol>
</li>
<li>
<ol start="3">
<li>rotate the plane so that x-axis becomes L</li>
</ol>
</li>
</ul>
</li>
<li>each of these is linear, so \(s_{L}\) is linear.</li>
<li>S3: in concrete terms, \(\theta : \) measured in radians, counter closckwise direction, Then, <ul>
<li>\(s_{L} = rot_{\theta} \circ s \circ rot_{-\theta}\)</li>
<li>use the previous theorem, and we can get matrices for these transformations:</li>
<li>\(A_{rot_{\theta}}\) = \(
            \begin{bmatrix}
            cos(\theta) &amp; -sin(\theta)  \\
            sin(\theta) &amp; cos(\theta)  \\
            \end{bmatrix}<br />
            \)</li>
<li>\(A_{rot_{\theta}}\) = \(
            \begin{bmatrix}
            cos(\theta) &amp; sin(\theta)  \\
            -sin(\theta) &amp; cos(\theta)  \\
            \end{bmatrix}<br />
            \)</li>
<li>\(A_{s}\) = reflection over x axis matrix </li>
<li>EXERCISE: derive these matrices </li>
<li>and so \(A_{s_{L}}\) = multiplication/composition of the three: = \(
            \begin{bmatrix}
            cos(2\theta) &amp; sin(2\theta)  \\
            sin(2\theta) &amp; -cos(2\theta)  \\
            \end{bmatrix}<br />
            \)</li>
</ul>
</li>
</ul>
</li>
<li>EXERCISE: Compute the matrix of the reflection over the line L with respect to the line L: y =7x in the standard basis of \(R^{2}\)</li>
</ul>
</li>
</ul>
<p><strong>Properties of the Composition of Linear Maps:</strong>   </p>
<ul>
<li>(1): if <ul>
<li>\( T \in L(V,W)\)</li>
<li>\( S \in L(U,V)\)</li>
<li>then \(T \circ S \in L(U,W)\)</li>
</ul>
</li>
<li>(2) Composition is a linear map in each argument. if<ul>
<li>\( S \in L(U,V) \) </li>
<li>then the map \( S \circ (-): L(V,W) \mapsto L(U,W) \) (no entiendo? QUESTION) is a linear map between the two vector spaces L(V,W), L(U,V)</li>
<li><em>example</em>: if \(a, b \in \mathbb{F}\), \(T_{1}, T_{2} \in L(V,W)\)</li>
<li>then \(S \circ (aT_{a} + b_{T_{2}})\)</li>
<li>= \( aS \circ T_{1} + bS \circ T_{2} \). </li>
<li><em>check</em>: we can check this directly! just check both sides are equal to the same thing when evaluated on any vect \(u \in U\) (because U is domain of S? That&rsquo;s where we start)</li>
<li>from the definition of addition and scaling in the vector space of linear maps, <a href="https://math.stackexchange.com/questions/2381942/the-set-of-all-linear-maps-tv-w-is-a-vector-space/2381955">here it is!</a></li>
<li>= \(S \circ (aT_{1}+bT_{2})(u)\)</li>
<li>= \(S \circ ((aT_{1}+bT_{2})(u))\)</li>
<li>= \(S \circ (aT_{1}(u) + bT_{2}(u))\)</li>
<li>= \( aS(T_{1}(u)) + bS(T_{2}(u))) \)</li>
<li>= \( a S \circ T_{1}(u) + b S \circ T_{2}(u)\)</li>
<li>QED</li>
<li>QUESTION:  CHECK WHETHER LINEAR MAPS IN QUESTION ARE CORRECT: THEY MIGHT BE FLIPPED IN THE NOTES! for this next one too</li>
<li>similarly: \(T \in L(V,W)\), check directly that \((-) \circ T : L(U,V) \mapsto L(U,W)\) is linear <ul>
<li>because \((aS_{1} + bS_{2}) \circ T = aS_{1} \circ T + b S_{2} \circ T\) for all a,b scalars and \(S_{1}, S_{2} \in L(U,V)\)</li>
</ul>
</li>
</ul>
</li>
<li>(3): Inverse maps of linear maps are linear. <ul>
<li>Suppose \(T: V \mapsto W\) is al inear map which is bijective (is invertible as a map of sets + is injective and surjective + is one to one and onto)</li>
<li>consider the inverse map \(T_{-1}: W \mapsto V\), </li>
<li>then \(T_{-1}\) is also linear. </li>
<li><em>proof</em>: let \(y_{1}, y_{2} \in W\) - the domain of inverse map, let \(a_{1}, a_{2} be two scalars in \mathbb{F}\)<ul>
<li>WTS: \( T_{-1}(a_{1}y_{1} + a_{2}y_{2}) = a_{1}T^{-1}(y_{1}) + a_{2}T^{-1}(y_{2}) \)</li>
<li>assign some names: \( x_{1} = T^{-1}(y_{1}) \in V \) and \(x_{2} = T^{-1}(y_{2}) \in V \) - V =codomain of \(T^{-1}\)  </li>
<li>then the RHS \( a_{1}T^{-1}(y_{1}) + a_{2}T^{-1}(y_{2}) \) is equal to \(a_{1}x_{1} + a_{2}x_{2} \in V\)</li>
<li>Let&rsquo;s evaluate the original map T on this vector. Remember T is linear and we get:</li>
<li>\( T(a_{1}x_{1} + a_{2}x_{2}) = a_{1}T(x_{1}) + a_{2}T(x_{2}) \)</li>
<li>= \( a_{1}T \circ T^{-1}(y_{1}) + a_{2}T \circ T^{-1}(y_{2})\)</li>
<li>= \( a_{1}y_{1} + a_{2}y_{2} \)</li>
<li>Thus, \(T(a_{1}x_{1} + a_{2}x_{2}) = a_{1}y_{1} + a_{2}y_{2}\)</li>
<li>Now let&rsquo;s evaluate \(T^{-1}\) on both sides of this identity. Literally just: </li>
<li>\(T^{-1} \circ T(a_{1}x_{1} + a_{2}x_{2})\) = \( T^{-1} (a_{1}y_{1} + a_{2}y_{2})\)</li>
<li>=&gt; \( a_{1}x_{1} + a_{2}x_{2}\) = \( T^{-1} (a_{1}y_{1} + a_{2}y_{2})\)</li>
<li>\( a_{1}T^{-1}(y_{1}) + a_{2}T^{-1}(y_{2})\) = \( T^{-1} (a_{1}y_{1} + a_{2}y_{2})\)</li>
<li>we have shown the linearity of the inverse map \(T^{-1}\)</li>
</ul>
</li>
</ul>
</li>
<li><strong>isomorphism</strong> (def): the linear map \(T : V \mapsto W\) is an <em>isomorphism</em> if there exists a linear map \(S: W \mapsto V\) s.t \(S \circ T = id_{v}\) and \(T \circ S = id_{v}\) (also think about corresponding definition in terms of matrices)<a href="https://math.stackexchange.com/questions/441758/what-does-isomorphic-mean-in-linear-algebra">more info</a><ul>
<li>note: we checked above that a linear map is an isomorphism iff it is a bijection </li>
<li>note: we also checked that the inverse linear map is simply the inverse set theoretic map.</li>
</ul>
</li>
<li><strong>isomorphism</strong> (def): two vector spaces V,W over \(\mathbb{F}\) are isomorphic if there exists a linear map \(T: V \mapsto W\) which is an isomorphism (so if it is bijective). <ul>
<li>isomorphic means &ldquo;has same shape&rdquo;. useful for turning undamiliar algebraic objects into familiar ones, making them easier to work with. </li>
<li>note: as with sets we will not distinguish isomorphic (vector) spaces. <ul>
<li>the rational is tht using the isomorphism and its inverse we can transport any property of V to W and back again</li>
<li>these preperties and feauees that only involce teh addition and scaling of vectors are matched in V and W via the isomorphism and its inverse. </li>
</ul>
</li>
</ul>
</li>
<li><strong>Examples of Isomorphisms and Isomorphic Spaces</strong>:</li>
<li><a href="https://en.wikibooks.org/wiki/Linear_Algebra/Definition_and_Examples_of_Isomorphisms">more examples</a></li>
<li>Example (1): (iso that sends linear combination vector to its coordinates and back)If V is a finite dimensional vector space over \(\mathbb{F}\) <ul>
<li>then every choice of a basis \(\mathbb{B} = \{ b_{1},..,b_{n} \}\) gives you an isomorphism, namely: \( [-]_{\mathbb{B}} : V \mapsto \mathbb{F} ^{n}\) between V and coordinate n space</li>
<li>so what does this map do?</li>
<li>\([-]_{\mathbb{B}}\) assigns to each \( v \in V\) <em>the column vector of its coordinates</em> in the basis \(\mathbb{B}\)</li>
<li>what this means is that each \( v \in V\) has a unique representation as \(v = \sum_{i=1}^{n} x_{i}b_{i}\) with \( (x_{1},..,x_{n}) \in V, b_{1}&hellip;b_{n} \in \mathbb{B} \) (in the vector space V!!)</li>
<li>this vector gets sent to a column vector of the coordinates from the field. \( [v]_{B} := (x_{1}&hellip;x_{n})^{T} \)</li>
<li>conversely, the inverse map \([-]^{-1} : \mathbb{F} ^{n} \mapsto V\) is also easy to write explicitly. Sends a column vector of coordinates from the scalar field to the linear combination representation of the vector v. </li>
</ul>
</li>
<li><em>Notation:</em> we can write and see the linear combination \(\sum_{i}^{n} x_{i}b_{i}\) as a formal matrix product \(\mathbb{B} \cdot x\) where<ul>
<li>\(\mathbb{B} = (b_{1},&hellip;,b_{n})\) == the row vector of vectors (in the basis)</li>
<li>\(x = (x_{1},&hellip;,x_{n})^{t}\) is the column vector of coordinates in \(\mathbb{F} ^{n}\)</li>
<li>with this new notation, new view, we can look at the inverse map of \([-]_{B}: V \mapsto F^{n}\) as the &ldquo;matrix multiplication by B map&rdquo;: \(B \cdot (-) : \mathbb{F} ^{n} \mapsto V\)</li>
</ul>
</li>
<li><em>further notes on example 1</em>: note that the isomorphism [-] depends on your choice of a basis. <ul>
<li>different bases will give rise to different isomophism. </li>
<li>another way to descrube \([-]_{\mathbb{B}} : V \mapsto \mathbb{F} ^{n}\) is to say that it is the unique linear map that sends the basis B of V to the standard basis \(E = \{e_{1},&hellip;,e_{n}\} \) of \(\mathbb{F} ^{n}\). <ul>
<li>so the mapping \([-]_{\mathbb{B}}\) is the unique linear map for which \( [b_{i}]_{\mathbb{B}} = e_{i}\) for i = 1,&hellip;,n</li>
<li>in vector notation : \( [\mathbb{B}]_{\mathbb{B}} = E \)</li>
</ul>
</li>
</ul>
</li>
<li>(2): Let V,W be vector spaces over \(\mathbb{F}\) with bases <ul>
<li>\(E = \{ e_{1},&hellip;,e_{n} \} \subset V\)</li>
<li>\(F = \{ f_{1},&hellip;,f_{n} \} \subset W\)</li>
<li>now given a linear map \(T: V \mapsto W\) let \(A_{T} \in Mat(mxn)(\mathbb{F})\) be its matrix (T&rsquo;s matrix) in the bases E and F. </li>
<li>Then the assignment \(A_{(-)}: L(V,W) \mapsto  Mat(mxn)(\mathbb{F}) :: T \mapsto A_{T}\) is a linear map. <ul>
<li>this follows from part (1) of the first theorem above. </li>
</ul>
</li>
<li>And \(A_{(-)}\) is also an isomorphism. (it&rsquo;s bijective)<ul>
<li><em>check</em>: <ul>
<li>first, by definition, \( A_{T} \) is the unique matrix such that \( T(E) = T(e_{1} + .. + T(e_{n})) = F \cdot A_{T} \) <ul>
<li>QUESTION: why is this true? are \(T(e_{1}, &hellip;)\) are column vectors?</li>
</ul>
</li>
<li>This fact guides us on how to compute values of T in terms of \(A_{T}\): (Here&rsquo;s how):<ul>
<li>if \(v \in V\) and we have \(x \in F^{n}\) := vector of coordinates of v in the basis E, </li>
<li>then \(v = Ex\)</li>
<li>by linearity of T we have: </li>
<li>\( T(v) = T(Ex) = T(E) \cdot x = F \cot A_{T}x\)</li>
<li>recalling that \(x = [v]_{E} \in F^{n}\) is the vector of the cordinates of v in the basis E, we now have a formula for T in terms of A_{T}, its matrix:<ul>
<li>\(T(v) = F A_{T}[v]_{E}\)</li>
</ul>
</li>
<li>this derivation also gives us the inverse of the map \(A_{(-)} : T \mapsto A_{T}\),<ul>
<li>\(A_{(-)} ^{-1} : Mat(mxn)(\mathbb{F}) \mapsto L(V,W) :: A \mapsto FA[-]_{\mathbb{E}}\) </li>
<li>QUESTION: so what does this actually look like? need examples</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Example (3): variant of example 1 (T that sends bases to bases is an iso)</li>
<li>Let V,W be vector spaces over \(\mathbb{F}\) with the bases:<ul>
<li>\(E = \{ e_{1},&hellip;,e_{n} \} \subset V \) </li>
<li>\(F = \{ f_{1},&hellip;,f_{n} \} \subset W \)</li>
</ul>
</li>
<li>Then the unique linear map \(T: V \mapsto W\) which sends the basis E to the basis F is an isomorphism!</li>
<li><em>Proof</em>: Let T be as defined above with bases E and F, for which \(T(E) = F\)</li>
<li>and let \(S: W \mapsto V\) be another such that \(S(F) = E\)</li>
<li>both T and S exist and are unique thanks to the theorem we proved above and lectures 7,8. </li>
<li>Then \(S \circ T : V \mapsto V\) is a linear map such that <ul>
<li>\(S \circ T(E) = S(T(E)) = S(F) = E\)</li>
</ul>
</li>
<li>But we already have such a map that maps from V to V and sends E to E. It&rsquo;s the identity map! \(id_{v}\)</li>
<li>by the uniqueness part of the above theorem and the last lecture, we get that this \(S \circ T = id_{v}\), they are one and the same. </li>
<li>the same argument applies to \(T \circ S\) on F, it equals \(id_{w}\) </li>
<li><strong>Corollary</strong>: Let V,W be finite dimensional vector spaces over F. Then <ul>
<li>(V and W are isomorphic) iff (dim V = dim W) <em>nice</em></li>
<li><em>proof</em>: =&gt; follows from isomorphism. look: <ul>
<li>If \(T: V \mapsto W\) is an isomorphism then for any basis B of V the vectors \(T(B) \subset W\) form the basis of W (important point!)</li>
<li>We can check this directly: <ul>
<li>if \(w \in W\) then we can consider \(v = T^{-1}(w) \in V\)</li>
<li>by definition, \(T(v) = T(T^{-1})(w) = w\). </li>
<li>but, \(v \in (V = span(B))\) </li>
<li>So by the linearity of T we have<ul>
<li>\(w = (T(v) \in span(T(B)))\) QUESTION: is spanT(B) = W? oh wait thats the conclusion we are trying to reach. </li>
</ul>
</li>
<li>This shows that T(B) is a generating set of W</li>
<li>But if \(B = \{ b_{1}&hellip;b_{n} \} \) and \( \{c_{1},..,c_{n}\} \) are scalars in the field such that the linear combination of these with the bases vectors is in W and equals 0, or \( c_{1}T(b_{1})+&hellip;+c_{n}T(b_{n}) = \vec{0} \in W\)</li>
<li>then we can apply \(T^{-1}\) to both sides and get: </li>
<li>\( T^{-1}(\vec{0}) = T^{-1}(\sum_{i=1}^{n} c_{i}T(b_{i})) \)</li>
<li>= \( \sum_{i=1}^{n} c_{i}T^{-1}T(b_{i}) \) (linearity of T)</li>
<li>= \( \sum_{i=1}^{n}c_{i}b_{i} \) </li>
<li>since B is a basis, this implies that \(c_{1}=&hellip;=c_{n} = 0\) and hence that T(B) is a linearly independent set of W</li>
<li>Since it is both spanning and linearly in dependent, </li>
<li>T(B) is a basis of W and thus&hellip;</li>
<li>dim V = number of vectors in B = number of vectors in T(B) = dim W</li>
</ul>
</li>
<li>(&lt;=): WTS: dim V = dim W =&gt; V and W and isomorphic <ul>
<li>supposed dim V = dim W = n (so there&rsquo;s n vectors in the basis)</li>
<li>and let <ul>
<li>\( E = {\ e_{1},..,e_{n} }\ \subset V\)</li>
<li>\( F = {\ f_{1},..,f_{n} }\ \subset W\) be bases</li>
</ul>
</li>
<li>Then by the previous claim(what is it!) the unique linear map \(T: V ]mapsto W\) such that T(E) = F (sends one basis to another) is an isomorphism. nice. </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><em>notation</em>: two isomorphic spaces over the same field:= \(V \simeq W\)</li>
<li><em>caution</em>: the previous claim just says if dim V = dim W there exists an isomorphism between them. It is not the case that any linear map between the two spaces is an isomorphism. <ul>
<li>ex: \(0 : v \mapsto W\) is a linear map between the two that is not an isomorphism. Not bijective when dim V is greater than 0. </li>
<li>the size of the Kernel helps tell if a mapping is an iso. </li>
</ul>
</li>
<li><strong>Kernel</strong> (definition) : the subspace of the domain of a linear mapping \(T : v \mapsto W\) between two vector spaces<ul>
<li>ker(T) = \( \{ v \in V s.t. T(v) = 0 \} \)</li>
</ul>
</li>
<li><strong>Theorem</strong> ( equal dimensions and kernel has zero vector implies iso ):<ul>
<li>given a linear map T between V and W, F vector spaces, that have the same finite dimension, </li>
<li>T is an iso iff &lt;=&gt; Ker(T) = {\( \vec{0} \)}</li>
<li><em>proof</em>: <ul>
<li>S1: ( =&gt; ) T is an isomorphism / bijective, so from the &ldquo;onto&rdquo; criteria we have <ul>
<li>there exists (for sure) a vector v in V that maps to 0 in W. This is because W needs to have the 0 vector to be a vector space. </li>
<li>there exists \(v \in V\) s.t. \(T(v) = \vec{0} \in W\)</li>
</ul>
</li>
<li>S2: But T is linear, so \(T(0) = T(0v) = 0 T(v) = 0\)</li>
<li>S3: This shows that \(\vec{0} \in V\) is the uniqe vector in V that is mapped to 0 in W by T. <ul>
<li>This proves that \(Ker(T) = 0\) </li>
<li>-(QUESTION: does this mean that the solution set has only the trivial solutin in an isomorphism?) - the id matrix is an iso, and when set = 0 its only solution is 0, think about this, so it seems to be the case, at least for the id matrix</li>
</ul>
</li>
<li>S1: (&lt;=) &ldquo;kernel only contains 0 implies T is an iso&rdquo; (harder!)<ul>
<li>suppose \(Ker(T) = \{\vec{0}\}\)</li>
<li>Let \(\{ e_{1},&hellip;,e_{n} \}\) be a basis of V</li>
<li>S2: so the vectors \(\{ f_{1} = T(e_{1} &hellip; f_{n} = T(e_{n})) \} \in W\) are linearly independent. (how do we know this?)<ul>
<li>well, if we have scalars \(a_{n} \in \mathbb{F}\) s.t. \(a_{1}f_{1},&hellip;,a_{n}f_{n} = \vec{0} \in W\) then we get</li>
<li>\(\vec{0} = \sum_{i=1}^{n}a_{i}f_{i}\)</li>
<li>\(= \sum_{i=1}^{n}a_{i}T(e_{i}) \)</li>
<li>= \( \sum_{i=1}^{n}T(a_{i}e_{i})\) (linearity of T)</li>
<li>= \( T(\sum_{a=1}^{n}a_{i}e_{i}) \) = \(\vec{0}\), (linearity of T)</li>
</ul>
</li>
<li>S3: but since we have that \(Ker(T) = \{\vec{0}\}\), <ul>
<li>\(Ker(T) = \{ \sum_{a=1}^{n}a_{i}e_{i} = \vec{0}\}\), and kernel is a subspace of the domain T, V, so \(\sum_{a=1}^{n}a_{i}e_{i} \in V \)</li>
</ul>
</li>
<li>S4: since {e}&rsquo;s were a basis of V, we have \(a_{2} = &hellip;= a_{n} = 0\) </li>
<li>S5: we have now shows that S2 is true. <ul>
<li>This face, that the {f} are linearly independent + dim V = dim W = n (bases of V and W have the same number of vectors) shows that \(f_{1},&hellip;,f_{n}\) is a basis of W. </li>
</ul>
</li>
<li>S6: Since T sends the basis of V to the basis of W, it is an isomorphism. </li>
<li>QED</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Image of T</strong>(definition): For a linear mapping \(T: V \mapsto W\),<ul>
<li>im(T) = \( \{w \in W : \exists v \in V s.t. T(v) = w\} \)</li>
<li>EXERCISE: check that im(T) is always a subspace of W.   </li>
</ul>
</li>
<li><strong>Theorem</strong>: V,W, F-vector spaces with equal finite dimension. dim V = dim W &lt; \(\infty\). Then a linear map \(T: V \mapsto W\) is an iso &lt;=&gt; (iff) im(T) = W </li>
<li>QUESTION: is this any different than the def of iso? being onto =&gt; range(T) = W? oh i see this only works for the first direction. <ul>
<li><em>proof</em> : ( =&gt; )If T is an isomorphism, then <ul>
<li>T is bijective (in particular onto/surjective:). Thus image(T) = W.</li>
</ul>
</li>
<li>(&lt;=) &ldquo;if im(T) = W then T is an isomorphism.&rdquo;</li>
<li>Suppose image(T) = W, </li>
<li>let \(\{ f_{1},&hellip;,f_{n}\}\) is a basis of W. </li>
<li>since im(T) = W we can find vectors \(e_{1},..,e_{n} \in V\) such that \(T(e_{i}= f_{i})\) , \(i = 1,&hellip;,n\)</li>
<li>Then the vectors \(\{e_{1},&hellip;,e_{n}\}\) are linerly independent in V. how so? well :<ul>
<li>suppose we have scalars \(a_{1},&hellip;,a_{n} \in K\) </li>
<li>so that \( a_{1}e_{1} + .. + a_{n}e_{n} = \vec{0} \)</li>
<li>then, \( T(\sum_{i=1}^{n} a_{i}e_{i}) = T(\vec{0})\)</li>
<li>\( \sum_{i=1}^{n} a_{i}T(e_{i}) = T(\vec{0})\) </li>
<li>\( \sum_{i=1}^{n} a_{i}f_{i} = T(\vec{0})\)</li>
<li>since \( \{ f_{1},&hellip;,f_{n} \} \) is a basis in V, then we can conclude that the scalars \( a_{1}=&hellip;=a_{n} = 0\)</li>
<li>thus, \( \{e_{1},&hellip;,e_{n}\} \) is linearly independent. </li>
</ul>
</li>
<li>Since we have dim V = dim W = n, this implies that \( \{ e_{1},&hellip;,e_{n} \} \) is a basis of V</li>
<li>and because we have a linear mapping T that sends a basis to another it is an isomorphism. </li>
<li>QED</li>
</ul>
</li>
<li>
<p>The argument we used to prove the previous two theorems can be refined to show that Ker(T) and im(T) always have complementary dimensions.    </p>
</li>
<li>
<p><strong>Theorem</strong>: (Rank-Nullity?) Let V,W be F-vector spaces. \( T:V \mapsto W\) is a linear map. Dimension V &lt; infinity (it&rsquo;s finite). <em>note that this only talks about the dimentison of the domain</em> Then:</p>
<ul>
<li>(a): dim Ker(T), dim Im(T) finite. </li>
<li>(b): dim Ker(T), dim Im(T) = dim V</li>
</ul>
</li>
<li><em>proof</em>: for part (a), note that \(Ker(T) \subset V\) is a subspace of V and by the monotonicity of dimension of spaces we get that <ul>
<li>\( dim(Ker(T) \leq dim(V) \leq \infty)\) (both finite, dim ker is less)</li>
</ul>
</li>
<li>Also if \( \{ e_{1},&hellip;,e_{n} \} \) is a basis of V<ul>
<li>then V = span\(\{e_{i}\}\) </li>
</ul>
</li>
<li>so then the image of T, which is all of T applied to every vector in V, is the span of T applied to every basis vector, by linearity of T:<ul>
<li>\(Im(T) = T(V) = span(T(e_{1}),&hellip;,T(e_{n})) \) </li>
</ul>
</li>
<li>from this fact it follows that \(im(T)\) is spanned by finitely many vectors, and so the dim \(im(T) \leq \infty\)</li>
<li>For part (b) : <ul>
<li>choose a basis of the Ker(T), \(\{ e_{1},&hellip;,e_{k} \}\).  </li>
<li>now choose a completion to a basis of V to a basis of V : \( \{ e_{1},&hellip;,e_{k},e_{k+1},&hellip;,e_{n} \} \)</li>
<li>QUESTION: get an explicit example of doing this!</li>
<li>Then we get that \(T(e_{1}),&hellip;,T(e_{n})\) span im(T)! QUESTION: why? from part a!</li>
<li>But \(T(e_{1}) = &hellip;= T(e_{k}) = 0\), from the basis of Ker(T)</li>
<li>so we get that really, \(T(e_{k+1}), &hellip;, T(e_{n})\) will span im(T)</li>
<li>but the vectors \(T(e_{k+1}), &hellip; , T(e_{n})\) will be linearly independent in \(im(T) \subset W\) (<em>how so</em>)? :<ul>
<li>well, if we have \(a_{k+1},&hellip;,a_{n} \in \mathbb{F}\) s.t. </li>
<li>\(\sum_{i=k+1}^{n} a_{i}T(e_{i}) = \vec{0}\) </li>
<li>then \( T(\sum_{i = k+1}^{n}a_{i}e_{i}) = \vec{0}\) (by linearity of T)</li>
<li>and so \( \sum_{i = k+1}^{n}a_{i}e_{i} \in Ker(T) \)<ul>
<li>because its the argument in T(0) = 0, do you see it?</li>
</ul>
</li>
<li>so \(\sum_{i = k+1}^{n}a_{i}e_{i} = \sum _{j=1}^{k}b_{j}e_{j}\)</li>
<li>since\(\{e_{1},&hellip;,e_{n}\}\) is a basis of V it follows that <ul>
<li>all coefficients must be equal to zero, \(a_{k+1},&hellip;,a_{n} = 0\), (which implies linear independence?)</li>
</ul>
</li>
<li>Thus \(T(e_{k+1}),&hellip;,T(e_{n})\) is a basis of im(T). </li>
<li>This shows that dim im(T) = \(n-k\) = dim V - dim ker(T). </li>
<li>QED</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="dual-spaces-and-isomorphisms"><strong>Dual Spaces and isomorphisms:</strong><a class="headerlink" href="#dual-spaces-and-isomorphisms" title="Permanent link"></a></h3>
<ul>
<li>We can use the criteria for isomorphic spaces to uncover more truths about the relationship between \(V \) and \(V^{v}\)  </li>
<li><strong>Claim</strong>: if V is a finite dimensional vector space over F, <ul>
<li>then (=&gt;) \(dimV^{v} = dim V\)</li>
</ul>
</li>
<li><em>proof</em>: Let \(E = \{e_{1},&hellip;,e_{n}\} \) be a basis of V. </li>
<li>Then for all \(i\) consider this unique linear operator / function:<ul>
<li>\({e_{i}}^{v}: V \mapsto \mathbb{F}\) (from the vector space to the field) s.t.</li>
<li>\({e_{i}}^{v}{e_{j}}\) = 
$$ \begin{cases} 
0 &amp; j \neq 1 \\
1 &amp; j = i 
\end{cases} $$</li>
</ul>
</li>
<li>we claim that the functions: \(e_{1}^{v},&hellip;,e_{n}^{v} \in V^{v}\) form a basis of the dual space \(V^{v}\) of \(V\) <em>how so you ask!</em></li>
<li>well, let \(f \in V^{v}\) be any element in the dual space, so it is a linear functional that sends vectors in v to the field. </li>
<li>the function \(f: V \mapsto \mathbb{F}\) is unqiely determined by its values on the spanning set E of V. <ul>
<li>QUESTION: so the lemma works for spanning sets, not just bases?????</li>
</ul>
</li>
<li>these values are: \( f(e_{1}),&hellip;,f(e_{n}) \in \mathbb{F} \). (recall they send vectors in v, in this case e, to a scalar in the field)</li>
<li>consider the linear function: <ul>
<li>g = \( f(e_{1})e_{1}^{v},&hellip;,f(e_{n})e_{n}^{v}\) </li>
<li>\(g(e_{j}) = f(e_{1})e_{1}^{v}(e_{j}) + &hellip; + f(e_{n})e_{n}^{v}(e_{j})\) = \(f(e_{j})\), because it becomes \( f(1*e_{j})\) for all j. </li>
<li>Thus, \(f : V \mapsto F\) and \(g : V \mapsto F\), take the same values on the basis \(e_{1},&hellip;,e_{n}\)</li>
<li>since linear transformations are characterized by what they do to a generating set, then \(f = g\) (remeber E is a spanning set of V)</li>
</ul>
</li>
<li>This shows that \(V^{v} =\) span\( (e_{1}^{v}, &hellip;, e_{n}^{v}) \)</li>
<li>Next, let scalars \(c_{1},&hellip;,c_{n}\) are such that \(c_{1}e_{1}^{v},&hellip;,c_{n}e_{n}^{v} = 0 \in V^{v}\)</li>
<li>so this \(c_{1}e_{1}^{v},&hellip;,c_{n}e_{n}^{v} : V \mapsto F\) is the zero function. </li>
<li>Evaluate the function on \( (e_{j})\), gives \( c_{1}e_{1}^{v}(e_{j}),&hellip;,c_{n}e_{n}^{v}(e_{j}) \)</li>
<li>but value of zero mapping on any vector just gives you zero back, so <ul>
<li>\( c_{1} = &hellip; = c_{n} =0\)</li>
</ul>
</li>
<li>Hence, \( \{ e_{1}^{v},&hellip;,e_{n}^{v} \}\) is linearly independent, thus they form a basis, since they are also spanning, of the dual space. </li>
<li>Hence, \(dim V^{v} = n = dim V\)  </li>
<li><strong>dual basis</strong> (def) : \(E^{v}= e_{1}^{v},&hellip;,e_{n}^{v} \), as defined above, is the dual bases of E (of V)</li>
<li><em>note</em>: dual spaces allow us (only when V is finite dimensional!) to define an isomorphism \(T: V \mapsto V^{v}\). <ul>
<li>T is defined as the unique linear map sending the basis E of V to the dual basis \(E^{v} of V^{v}\).</li>
</ul>
</li>
<li>also works in opposite direction &hellip; </li>
<li>
<p><strong>Claim</strong>: V is a finite dim vector space over F, let</p>
<ul>
<li>\( F = \{ f_{1},&hellip;,f_{n} \} \) be a basis of \(V^{v}\)</li>
<li>Then there exists a unique basis \(E = \{ e_{1},&hellip;,e_{n} \}\) of V s.t.\(E^{v} = F\) </li>
<li>(a basis for the dual space can always constructed from a basis for the space, so we can find the &ldquo;origin&rdquo; basis for any basis in \(V^{v}\))</li>
<li><em>proof</em>: <ul>
<li>note that each vector \( x \in V \) defines a linear function <ul>
<li>\(ev_{x}: V^{v} \mapsto \mathbb{F}\) alt: \(f \mapsto f(x)\) (the evaluation map! kirillov was talking about this!)</li>
</ul>
</li>
<li>This gives a map: <ul>
<li>\(ev : V \mapsto V^{vv}\) alt: \( x \mapsto ev_{x} \)</li>
</ul>
</li>
<li>this map is linear:<ul>
<li>if \(x_{1},x_{2} \in V\), \(a_{1}, a_{2} \in F\)</li>
<li>then for every \( f \in V^{v}\) we have: </li>
<li>\(ev_{a_{1}x_{1} + a_{2}x_{2}}(f) =V s\mapsto F f(a_{1}x_{1}+a_{2}x_{2}) = a_{1}f(x_{1}) + a_{2}f(x_{2}) = a_{1}ev_{x1}(f) + a_{2}ev_{x2}(f)\)</li>
</ul>
</li>
<li>By the previous claim on \(dim V^{vv} = dim V^{v} = n\),</li>
<li>so \(ev: V \mapsto V^{vv}\) is a linear map between vector spaces of the same dimension. </li>
<li><em>now a brief interruption for a lemma we need to prove to finish proving this claim</em>:</li>
</ul>
</li>
<li><strong>Lemma:</strong> (the evaluation map is an isomorphism)<ul>
<li>For a finite dimensional vector space V over F the map \( ev: V \mapsto V^{vv} \) is an isomorphism. (a natural one!)</li>
<li><em>proof</em>: Let \(\{ b_{1},..,b_{n} \}\) be a basis of \(V\). <ul>
<li>let \(\{b_{1}^{v},&hellip;,b_{n}^{v}\}\) be a basis \(V^{v}\). </li>
<li>let \( \{ b_{1}^{vv},&hellip;,b_{n}^{vv} \} \subset V^{vv}\) be a basis dual to the dual basis. </li>
<li>Compute \(ev_{b_{i}}(b_{j}^{v})\). </li>
<li>By definition, we have </li>
<li>\( ev_{b_{i}}(b_{j}^{v}) = b_{j}^{v}(b_{i}) =  \)
$$ \begin{cases} 
0 &amp; j \neq 1 \\
1 &amp; j = i 
\end{cases} $$</li>
<li>Thus, we can see that \( ev_{bi} \) and \( b^{vv} \) take on the same values on the dual basis \( \{ b_{1}^{v},&hellip;,b_{n}^{v} \} \) </li>
<li>since every linear map,function is uniquely detemined by its values on a spanning set, it follows that:<ul>
<li>\( ev_{bi} = b_{i}^{vv}\) for i =1,..,n</li>
</ul>
</li>
<li>Conclusion: \(ev\) sends a basis of V toa basis of \(V^{vv}\) and so it is an isomorphism. </li>
</ul>
</li>
</ul>
</li>
<li>back to the proof.... </li>
<li>Now let us start wuth the basis<ul>
<li>\( F = \{ f_{1},&hellip;,f_{n}\} \) of \(V^{v}\)</li>
</ul>
</li>
<li>Let \(F^{v} = \{ f_{1}^{v},&hellip;,f_{n}^{v} \}\) be the dual basis of \(v^{vv}\)</li>
<li>Since we have \(ev: V \mapsto V^{vv}\) is an isomorphism (proved above)</li>
<li>for each \(i = 1,&hellip;,n\) we have a unique vector \(e_{i} \in V\) s.t.<ul>
<li>\(ev_{e_{i}} = f_{i}^{v}\) (because an iso sends a basis to a basis?)</li>
</ul>
</li>
<li>But then, \(f_{i}(e_{j}) = ev_{ej}(f_{i}) = f_{j}^{v}(f_{i}) = \begin{cases} 
        0 &amp; j \neq 1 \\
        1 &amp; j = i 
        \end{cases} \)</li>
<li>Now consider the collection of vectors \( \{ e_{1},&hellip;,e_{n} \} \subset V\)<ul>
<li>It is a basis of V since it is the image of the basis of the double dual: \(\{ f_{1}^{v},..,f_{n}^{v} \}\) of \(V^{vv}\) under the isomorphism: \(ev^{-1}: V^{vv} \mapsto V\)</li>
</ul>
</li>
<li>The formula \( f_{i}(e_{j}) = \begin{cases} 
        0 &amp; j \neq 1 \\
        1 &amp; j = i 
        \end{cases} \) implies that ..</li>
<li>\(f_{1},..,f_{n}\) is the dual basis of \( e_{1},&hellip;,e_{n} \) and this proves our claim. </li>
<li>QED   </li>
</ul>
</li>
<li>
<p><em>Remark</em>: If \(V\) is an n-dimensional space over F, we now know that </p>
<ul>
<li>\(V \simeq V^{v}\) </li>
<li>\(V \simeq V^{vv}\)</li>
</ul>
</li>
<li>However, the first one is a isomorphism that depends on the choice of the basis, the second is a <em>canonical</em> isomorphism.</li>
<li>To construct the first, we need to find a basis of V. </li>
<li>To construct the second, there is a canonical isomorphism \(ev: V \mapsto V^{vv}\) which is deinfed only in terms of V and doesn&rsquo;t depend on a basis. </li>
</ul>
<p>** <strong>Exercises:</strong>
* (Duality for Linear Maps): Let U, V, W be vector spaces over F
    - (1) Show that if \(T: V \mapsto W\) is a linear map, then the map \(T^{v}: W^{v} \mapsto V^{v}\) defined by \(T^{v}(f) = f \circ T\), for any linear function \(f: W \mapsto F\) is also linear. 
    - (2) Show that if \(S: U \mapsto V\), \(T: V \mapsto W\) are linear maps, then \( (T \circ S)^{v}: S^{v} \circ T^{v}\)
    - (3) Suppose that V,W are finite dimensional and let 
        + \( E = \{ e_{1},&hellip;,e_{n} \} \subset V \)
        + \( F = \{ f_{1},&hellip;,f_{n} \} \subset W \) 
    - be bases of V and W. 
    - Let \(S \in Mat(mxn)(F)\) be the matrix of T in the bases \(E \subset V\) and \( F \subset W \). 
    - Show that the matrix of \(T^{v}: W^{v} \mapsto V^{v}\) in the dual bases \( F^{v} \subset W^{v} \) and \( E^{v} \subset V^{v} \) is the matrix \( A^{T} \in Mat(nxm)(F) \)
    - (4) Prove that if \(A \in Mat(nxm)(F)\) and \(B \in Mat(mxk)(F)\)
        + then \((AB)^{T} = B^{T}A^{T}\). </p>
<hr />
<h3 id="lectures-12-13">Lectures 12-13<a class="headerlink" href="#lectures-12-13" title="Permanent link"></a></h3>
<ul>
<li>
<h4 id="traces"><strong>Traces</strong>:<a class="headerlink" href="#traces" title="Permanent link"></a></h4>
</li>
<li>It is useful to have numerical invariants measuring the complexity of linear maps  </li>
<li>
<p>we already have some discrete (= integer invariants)  </p>
<ul>
<li>for every linear map \( T: V \mapsto W \)  </li>
<li>we have two integers cpaturing information about T (transformation)  <ul>
<li>
<h5 id="nullity-of-t-dim-kernelt-dim-nullspacet-dim-of-the-solution-set-to-92ax092"><strong>nullity of T:</strong> = dim Kernel(T) = dim Nullspace(T) = dim of the solution set to \(Ax=0\)<a class="headerlink" href="#nullity-of-t-dim-kernelt-dim-nullspacet-dim-of-the-solution-set-to-92ax092" title="Permanent link"></a></h5>
<ul>
<li><strong>Nullspace (T)</strong>: set of all n-dimensional column vectors such that \(Ax=0\), the solution set of the homog linear system.  <ul>
<li><strong><em>Theorem</em></strong>: The nullspace N(A) is a subspace of the vector space \(\mathbb{R^{n}}\)  </li>
<li>proof: WTS: N(A) is nonempty, closed under addition, closed under scalar multiplication:  </li>
<li>S1: the trivial solution is always in N(A)- so it&rsquo;s nonempty. \(\vec{x}=\vec{0}\)  </li>
<li>S2: WTS: \( x,y \in N(A) =&gt; x+y \in N(A)\)  <ul>
<li>Well, \( Ax = 0, Ay = 0, A(x+y) = A(x) + A(y) = 0 + 0 = 0 \)  </li>
</ul>
</li>
<li>S3: \(c \in \mathbb{R}, x \in N(A) =&gt; cx \in N(A)\)  <ul>
<li>Well, \(A(cx)=c*A(x) = c * 0 = 0\)</li>
</ul>
</li>
<li>QED  </li>
</ul>
</li>
</ul>
</li>
<li><strong>rank of T</strong>: dim image(T) = &hellip;QUESTION: any other defs? yes, see lec 14 for longer discussion </li>
</ul>
</li>
</ul>
</li>
<li>
<p>turns out that for linear operators \(T: V \mapsto V\) we also have defined invariants which are scalars of the field \(\mathbb{F}\)  </p>
<ul>
<li>ex: <strong>Trace</strong>: \(tr: L(V,V) \mapsto \mathbb{F}\) is  <ul>
<li>the sum of elements on the main diagonal of a square matrix A  </li>
<li>the sum of its complex eigenvalues  </li>
<li>invariant with respect to change of basis  </li>
<li>trace with this def applies to linear operators in general</li>
<li>is a linear mapping: \( tr(T + S) = tr(T) + tr(S)\) and \( tr(cT)= c * tr(T) \)</li>
<li>notice inside L(V,V) (linear maps from V to V) we have a natural collection of linear operators, from each one we can get a scalar back. <ul>
<li>how can we get this scalar? </li>
<li>given any pair (f,v) where <ul>
<li>\(v \in V\) is a vector</li>
<li>\(f \in V^{v}\) is a linear functional in the dual space = the space of all linear functionals from V to the scalar field</li>
</ul>
</li>
<li>we can construct a linear operator:<ul>
<li>\(s_{f,v}: V \mapsto V, x \mapsto f(x)v \)<ul>
<li>QUESTION: doesnt this give me a vector back?</li>
</ul>
</li>
</ul>
</li>
<li>but given (f,v) we can also get a natural scalar: <ul>
<li>\( f(v)\in \mathbb{F} \)</li>
</ul>
</li>
<li>with this in mind we can form and prove the existence statement:</li>
<li><strong><em>Lemma</em></strong>: <ul>
<li>Suppose V is finite dim vector space over \(\mathbb{F}\)</li>
<li>Then there exists a unique linear function:<ul>
<li>\(tr: L(V,V) \mapsto \mathbb{F}\)</li>
<li>such that for all \(v \in V\) and \(f \in V^{v}\)</li>
<li>\(tr(s_{f,v}) = f(v)\)</li>
</ul>
</li>
</ul>
</li>
<li>proof of lemma: <ul>
<li>fundamental fact: every linear function (any linear transformation) is uniquely determined by what it does to a basis (by its values on a basis)</li>
<li>from this fact, it suffices to constrct a basis of all linear functions from V to V, \(L(V,V)\) that consists of operators of the form \(s_{f,v}\) for the chosen f&rsquo;s and v&rsquo;s<ul>
<li><a href="https://math.stackexchange.com/questions/2619275/whats-a-basis-for-mathcal-lv-w">more here</a> </li>
</ul>
</li>
<li>Let \( \mathbb{B} = { b_{1}.......b_{n} } \subset V \) be any basis of V</li>
<li>Let \( \mathbb{B}^{v} = {b_{1}^{v}.......b_{n}^{v} } \subset V \) be its dual basis</li>
<li>Then we can say that the collection of operators <ul>
<li>\( \mathbb{S} = { s_{{b_{1}}^{v},b_{1}}.......s_{{b_{n}}^{v},b_{n}} } \) is a basis of \(L(V,V)\) the set of all linear functions from V to V<ul>
<li>basis = spanning + linearly independent. </li>
<li>here, each \({b_{i}}^{v}\) is a linear functional from the dual basis, and each \(b_{i}\) is a vector from the basis of V. Each gets plugged into the linear operator s and spits out a and spits out a \( {b_{i}}^{v} * b_{i} \), which is QUESTION: a vector in V?</li>
</ul>
</li>
<li>proof that \( \mathbb{S} \) is a basis for L(V,V): <ul>
<li>S1: \( T: V \mapsto V \) is a linear map.  <ul>
<li>Let \( A \in Mat_{nxn}\mathbb{F} \) be the matrix of T in the basis \( \mathbb{B} \)<ul>
<li>note: we can always represent a linear transformation/mapping by a matrix in its</li>
</ul>
</li>
</ul>
</li>
<li>S2: Then \( T =  \sum_{i,j=1}^{n} a_{ij} * s_({{b_{ji}}^{v}, b_{i}}) \) <ul>
<li>for every \( k = 1,....,n \), we have</li>
<li>\( T(b_{k}) =  \sum_{i=1}^{n} a_{ik} * b_{i} \)</li>
<li>and we also have:</li>
<li>\( (\sum_{ij} a_{ij}*s_{{b_{ji}}^{v}b_{i}} )(b_{k}) \) </li>
<li>\( =  {\sum_{ij}} a_{ij}*s_{{b_{ji}}^{v}b_{i}}  (b_{k}) \)</li>
<li>\( =  \sum_{ij} a_{ij} {b_{j}}^{v}(b_{k})b_{i} \)</li>
<li>\( = \sum_{i=1}^{n} a_{ik} * b_{i} \)</li>
</ul>
</li>
<li>S3: Thus, \( T = \sum_{ij}a_{ij}s_{{b_{j}}^{v}, b_{i}} \)</li>
<li>This representation is unique since the matrix of T in the basis \( \mathbb{B} \) is uniquely determined by T and \( \mathbb{B} \) 
    *(&ldquo;linear extension theorem&rdquo; - a linear transformation is uniqely determined by what it does to a basis.) ?</li>
<li>by the characterization of linear maps (the one descirbed above?) we then have a unique linear function, trace:<ul>
<li>\( tr: L(V,V) \mapsto \mathbb{F} \)</li>
<li>ENDED PG 7 REVISIT AFTER LEC 9-11</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="lecture-14-row-reduction">Lecture 14: Row Reduction<a class="headerlink" href="#lecture-14-row-reduction" title="Permanent link"></a></h3>
<p><strong>Outline:</strong></p>
<ul>
<li>Simplifying Linear Systems</li>
<li>Row Reduction and Echelon Forms</li>
<li>Solving Systems with Row Reduction</li>
<li>Corollaries</li>
<li>
<h4 id="solving-a-linear-system"><strong>Solving a Linear System</strong><a class="headerlink" href="#solving-a-linear-system" title="Permanent link"></a></h4>
</li>
<li>using row and column operations we can convert every linear system into a system in which all variables separate<ul>
<li><em>row operation</em>: </li>
<li><em>column operation</em>:</li>
</ul>
</li>
<li>\(Ax=b\) where <ul>
<li>\(A \in Math(mxn)(F)\) is a given coefficient matrix. </li>
<li>\(b \in F^{m}\) is a given vector of right hand sides. </li>
<li>\(x \in F^{n}\) is an unknown vector. </li>
</ul>
</li>
<li>we proved (QUESTION: where b?) that we can find invertible matrices<ul>
<li>\( R \in Mat(mxm)(F) \)  - number of eqs</li>
<li>\( C \in Mat(nxn)(F) \) - number of unkowns</li>
<li>such that after performing on A the row operation corresponding to R, and then the column operation corresponding to C we get the simplest possible matrix:</li>
<li>\( \tilde{A} =  RAC =\) <img alt="block matrix" src="/C:/Users/Lisette%20del%20Pino/Documents/github_blog_site/delpinolisette.github.io/delpinolisette.github.io/img/blockmat1.png" /><ul>
<li>where r = rank of matrix (see discussion on rank below!!)</li>
</ul>
</li>
<li>Using R and C we can simplify the system Ax=b. </li>
<li>Left multiplying (Ax=b) by R gives an equivalient system <ul>
<li>\(RAx = Rb &lt;=&gt; RACC^{-1}=Rb\)<ul>
<li>so the id matrix is written in the form \(CC^{-1}\) which doesnt change anything, but we have our simple matrix RAC to work with. </li>
</ul>
</li>
<li>given the notation: <ul>
<li>\(\tilde{A} = RAC \in Mat(mxn)F\)</li>
<li>\(\tilde{b} = Rb \in (F^{m})\)</li>
<li>\(\tilde{x} = C^{-1}x \in F^{n} \) = column vector with n entries. </li>
</ul>
</li>
<li>write system as \(\tilde{A}\tilde{x}=\tilde{b}\), but since we used only invertible operations to get this new system, we can answer quesetions about the old system with this one. </li>
<li>in terms of the new variables \(\tilde{x} = (\tilde{x}_{1},&hellip;,\tilde{x}_{n})^{T}\) = \(C^{-1}x\)</li>
<li>so it becomes : 
\[ \tilde{x}_{1} = \tilde{b}_{1} \\
&hellip; \\
\tilde{x}_{r} = \tilde{b}_{r} \\
0 = \tilde{b}_{r+1}\\
&hellip;\\
0 = \tilde{b}_{m}
\]</li>
<li>note that it is m equations. we got this very simple system we alluded to using the block matrix previously. </li>
<li>So we conclude that we can get the solutions of Ax=b through \(\tilde{A}\tilde{x}=\tilde{b}\)<ul>
<li><em>proof</em>: </li>
<li>S1: The tilde system and Ax = b are both consistent iff \(\tilde{b}_{r+1}= &hellip; = \tilde{b}_{m}=0\)</li>
<li>S2: If \( \tilde{b}_{r+1}=&hellip;=\tilde{b}_{m}=0\) then the solutions of \( \tilde{A}\tilde{x}=\tilde{b} \) are vectors of the form: \[ \tilde{b}_{1} \\
 &hellip; \\
\tilde{b}_{r} \\
\tilde{x}_{r+1} \\
&hellip; \\
\tilde{x}_{n}  \]</li>
<li>with \( \tilde{x}_{r+1},&hellip;,\tilde{x}_{n} \in \mathbb{F} \) being free variables</li>
<li>S3: If \(\tilde{b}_{r+1}= &hellip; =\tilde{b}_{m}=0\) then the solutions of \(Ax=b\) are of the form: x = 
\[ \tilde{b}_{1} \\
 &hellip; \\
\tilde{b}_{r} \\
\tilde{x}_{r+1} \\
&hellip; \\
\tilde{x}_{n}  \]</li>
<li>with \( \tilde{x}_{r+1},&hellip;,\tilde{x}_{n} \in \mathbb{F} \) being free variables.\, and the whole column vector of solutions is multiplied by \(C\). (why? because solutions \(\tilde{x} were our solutions x multiplied by C^{-1}\) !)</li>
</ul>
</li>
<li>but this method is not constructive, we had to pick bases in \(F^{n}, F^{m}\) to fit with A. We had actually construct R and C by choosing these bases, adopted to map T, or matrix of map A. Row reduction algorithm solves the issue. <ul>
<li>the algo simplifies the system systematically, uses only row ops, done in simple steps, and allows us to get close enough to solve the system.  </li>
<li>also they solve the systm in <em>finitely</em> many steps!</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<h4 id="elementary-row-operations"><strong>Elementary Row Operations</strong>:<a class="headerlink" href="#elementary-row-operations" title="Permanent link"></a></h4>
<ul>
<li>note that Ax=b is the matrix equation for our equation \(F(v) = b\), once we choose a basis in the vector space V. </li>
<li>If A is (mxn), the augmented matrix is (m x n+1). </li>
<li>Elemtary Row Operations are just left multiplication by a specific elementary matrix. We called it R above. <ul>
<li>they just replace rows in the matrix with linear combinations of rows, in an invertible way (you can always work backwards).</li>
<li>The important question is - why is it that these row operatiors don&rsquo;t change the solution set?? Because they are all invertible, reversible. </li>
<li>the inverses of ERO&rsquo;s are just ERO&rsquo;s of the same type. </li>
<li><img alt="ERO's and inverses:" src="/C:/Users/Lisette%20del%20Pino/Documents/github_blog_site/delpinolisette.github.io/delpinolisette.github.io/img/ERO.png" /></li>
<li>What are these special elementary matrices? Well, they are just obtained by doing the corresponding row operation to the identity matrix. <ul>
<li>Row operation 1: switch two rows: <img alt="type 1" src="/C:/Users/Lisette%20del%20Pino/Documents/github_blog_site/delpinolisette.github.io/delpinolisette.github.io/img/type1.png" /></li>
<li>Row operation 2: multiply a row by a scalar multiple: <img alt="type 2" src="/C:/Users/Lisette%20del%20Pino/Documents/github_blog_site/delpinolisette.github.io/delpinolisette.github.io/img/type2.png" /></li>
<li>Row operation 2: add a scalar multiple of a row to another row: <img alt="type 3" src="/C:/Users/Lisette%20del%20Pino/Documents/github_blog_site/delpinolisette.github.io/delpinolisette.github.io/img/type3.png" /></li>
</ul>
</li>
<li>another way of showing that a row operation does not change the solution set of a system, matrix equation style :) E is the elementary matrix representating a row operation.<ul>
<li>\(Ax=b\)</li>
<li>\(EAx = Eb\) (implies any solution of this equation is a solution of the previous, because we multiplied them all by the same matrix)</li>
<li>\(E^{-1}EAx = E^{-1}Eb\)</li>
<li>\(Ax = b\), QED. </li>
</ul>
</li>
<li><strong>claim</strong>: the composition of row operations is also a row operation! <ul>
<li><em>proof</em>: if \(p_{1},&hellip;,p_{s}: \mathbb{F}^{n} \mapsto \mathbb{F}^{n}\) </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<h4 id="rank-of-a-matrix"><strong>Rank of a Matrix</strong> :<a class="headerlink" href="#rank-of-a-matrix" title="Permanent link"></a></h4>
<ul>
<li>recall that left multiplication by an mxn matrix \(A\) defines a linear map: <ul>
<li>\(T : \mathbb{F}^{n} \mapsto \mathbb{F}^{m}\) :: \(v \mapsto Av\)</li>
</ul>
</li>
<li>the rank of the matrix A is then the rank of T. <ul>
<li>rank(T) = dimension of the subspace \( im(T) \subset \mathbb{F}^{m}\) of the codomain. </li>
</ul>
</li>
<li><strong>rank (A)</strong> (def): rank (A) = rank(T) = dim im(T) = dim (column space of A) = maximal number of linearly independent vectors in col space A = max number of linearly independent columns of A. </li>
<li><strong>im(T)</strong> (def): im(T) = \(y \in F^{m} : y = T(v) = Av\) for some \(v \in F^{n}\) = span(columns of A) <ul>
<li>(we show the last part now), that im(T) = span of columns of A.</li>
<li>now note that: </li>
<li>\(F^{n}\), the domain of the linear map, which is a coordinate n space, is spanned by the vectors of the cardinal basis \(e_{1},&hellip;,e_{n}\) and therefore \(im(T)\) is spanned by the vectors \(Ae_{1},&hellip;,Ae_{n} \in F^{m}\), which as we now discuss are just the columns of A<ul>
<li>we know that image of a linear map is gneerated by the image of the generators of the source space (QUESTION: - when did we discuss this?)</li>
<li>recall that each \(e_{i}\) has a 1 in the ith spot, 0 everywhere else, so by definition of matrix multiplication (row by column), <ul>
<li>\(Ae_{i}\) = i-th column of A. </li>
</ul>
</li>
</ul>
</li>
<li>Thus, im(T) = span(columns of A)</li>
</ul>
</li>
<li>To be continued</li>
</ul>
</li>
</ul>
<hr />
<h3 id="chapter-1">Chapter 1<a class="headerlink" href="#chapter-1" title="Permanent link"></a></h3>
<hr />
<h3 id="chapter-2">Chapter 2<a class="headerlink" href="#chapter-2" title="Permanent link"></a></h3>
<h4 id="section-23-analyzing-the-pivots">Section 2.3 : Analyzing the Pivots:<a class="headerlink" href="#section-23-analyzing-the-pivots" title="Permanent link"></a></h4>
<ul>
<li>All questions about the existence of a solution + uniqueness of a solution of a system can be answered by : look at the <em>pivots</em> in RREF form of the augmented matrix of the system. (needs to be \(A^{rref}\))!</li>
<li><strong>when is \(Ax=b\) inconsistent?</strong><ul>
<li>when it doesn&rsquo;t have a solution!</li>
<li>(1) a system is <strong>inconsistent</strong> (&lt;=&gt;) (iff) <ul>
<li>there is a pivot in the last row of the augmented matrix. </li>
<li>so it looks like (\(0 0 0 0 : 1\)) <ul>
<li>this implies the last equation is \(0x_{1} + &hellip; + 0x_{n} = b, b\neq 0\), </li>
<li>=&gt; no solution. </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>remarks on the coefficient matrix</strong>: <ul>
<li>(1): a solution, if it exists, is unique \(\iff\) : <ul>
<li>there are no free variables </li>
<li>the echelon form of the coefficient matrix has a pivot in each <em>column</em>, </li>
<li>(note that this says nothing about the existence, only uniqueness of solution.)</li>
<li>QUESTION: how does this connect to rank, and the fact that rank(A) = dim column space of A?</li>
</ul>
</li>
<li>(2): \(Ax = b\) is consistent (has a solution) for all b \(\iff\) : <ul>
<li>the echelon form of the <em>coefficient matrix</em> has a pivot in each_row_</li>
<li>note: this property makes sense considering if there is a pivot in each row of coefficint matrix, you can&rsquo;t have a pivot in the last column on the augmented matrix. =&gt; never inconsistent. </li>
</ul>
</li>
<li>(3): Ax = b has a <em>unique solution</em> (and it exists..) for all b iff \(\iff\):<ul>
<li>echelon form of the coefficient matrix \(A\) has a pivot in each row and each column. </li>
</ul>
</li>
</ul>
</li>
<li><strong>remarks on inconsistency</strong> (very cool)<ul>
<li>WTS: To be continued</li>
</ul>
</li>
</ul>
<h4 id="section-27-fundamental-subspaces-and-rank">Section 2.7 : Fundamental Subspaces and Rank:<a class="headerlink" href="#section-27-fundamental-subspaces-and-rank" title="Permanent link"></a></h4>
<ul>
<li>four fundamental subspaces:<ul>
<li>Ker(A), Ran(A), row space(A), Left nullspace(A)</li>
<li>need to study relationships between their dimensions. </li>
</ul>
</li>
<li>any linear transformation has subspaces : Ker(A), Range(A)<ul>
<li><strong>Ker(A)</strong>, subset of domain \(V\) : <ul>
<li>Ker(A) = Nullspace(A) := \(\{ v \in V : Av = 0 \} \subset V\)</li>
<li>solution set of the homogenous equation Ax = 0</li>
<li>Ker(\(A^{T}\)) = left null space. </li>
</ul>
</li>
<li><strong>Ran(A)</strong>, subset of codoamin \(W\) :<ul>
<li>Range(A) := \(\{w \in W : w = Av, v \in V\}\)</li>
<li>set of all right sides \(b \in W\) for which \(Ax = b\) has a solution, &ldquo;is consistent.&rdquo;</li>
<li>Range(A) = column space(A) (a basis is pivot columns&rsquo; originals)</li>
<li>Range(\(A^{T}\)) = row space (a basis is pivot rows)</li>
</ul>
</li>
<li><strong>rank (A)</strong> = dim Range(A) = dim Colspace(A)</li>
</ul>
</li>
<li>Computing fundamental subspaces: <ul>
<li>S1: reduce \(A\) to \(A_{e}\) (echelon form of A). </li>
<li>S2: <em>original</em> columns of \(A\) which becomes pivot columns of \(A_{e}\) are a <em>basis</em> in Range(A). </li>
<li>S3: pivot rows of \(A_{e}\) are a basis for rowspace(A)<ul>
<li>you can also transpose the row matrix and do row operations, but easier this way. </li>
</ul>
</li>
<li>S4: basis for Ker(A) = nullspace(A) comes from solving homogenous \(Ax = 0\). <ul>
<li>Consider \(A\) and \(A_{e}\):</li>
<li>A = \(  \begin{bmatrix}
        1 &amp; 1 &amp; 2 &amp; 2 &amp; 1\\
        2 &amp; 2 &amp; 1 &amp; 1 &amp; 1 \\
        3 &amp; 3 &amp; 3 &amp; 3 &amp; 2 \\
        1 &amp; 1 &amp; -1 &amp; -1 &amp; 0
        \end{bmatrix}\), \(A_{e}\) = \( \begin{bmatrix}
                                            1 &amp; 1 &amp; 2 &amp; 2 &amp; 1 \\
                                            0 &amp; 0 &amp; -3 &amp; -3 &amp; -1 \\
                                            0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
                                            0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
                                            \end{bmatrix}\)</li>
<li>since columns \(1,3\) becomes pivot columns in \(A_{e}\), then the columns 1 and 3 of \(A\) are a basis for colspace(A) = Ran(A). (notice that the dimension of the colspace/ran(A) is 2.)</li>
<li>since rows \(1,2\) are pivot rows of \(A_{e}\), they form a basis for rowspace(A), in their REF. (notice dim rowspace(A) = 2). </li>
<li>now, solving in REF, we get the solution set of A, which can be written in vector form \[ \begin{bmatrix}
                            -t-(1/3)r \\
                            t \\
                            -s-(1/3)r \\
                            s \\
                            r
                            \end{bmatrix} \]</li>
<li>or as \(\begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}\) t + \(\begin{bmatrix} 0 \\ 0 \\ -1 \\ 1 \\ 0 \end{bmatrix}\)s + \(\begin{bmatrix} -(1/3) \\ 0 \\ (1/3) \\ 0 \\ 1 \end{bmatrix}\)r. <ul>
<li>these are then the three basis vectors, and the dimension of the kernel is 3. </li>
<li>note that there is no shortcut for computing Ker(\(A^{T}\)), you need to solve \(A^{T}x = 0\) by hand. </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Explanation for computing bases of fundamnetal subspaces</strong>: why do these methods work ?<ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<h2 id="chapter-3">Chapter 3<a class="headerlink" href="#chapter-3" title="Permanent link"></a></h2>
<hr />
<h2 id="chapter-4-spectral-theory">Chapter 4: Spectral Theory<a class="headerlink" href="#chapter-4-spectral-theory" title="Permanent link"></a></h2>
<hr />
<h2 id="chapter-8-dual-spaces-and-tensors">Chapter 8: Dual Spaces and Tensors<a class="headerlink" href="#chapter-8-dual-spaces-and-tensors" title="Permanent link"></a></h2>
<ul>
<li>all spaces here are finite dimensional</li>
<li>might wanna refer to <a href="#dual-spaces-and-isomorphism">Dual Spaces and Isomorphisms</a></li>
</ul>
<h3 id="section-81-dual-spaces">Section 8.1 : Dual Spaces<a class="headerlink" href="#section-81-dual-spaces" title="Permanent link"></a></h3>
<ul>
<li><strong>Definition: Linear Functional</strong> :<ul>
<li>a special type of linear transformation on a vector space \(V\) that sends a vector from the vector space to a scalar in the field, \(L: V \mapsto \mathbb{F}\)</li>
</ul>
</li>
<li>Examples?<ul>
<li>a velocity vector in a given direction (physical object) getting mapped to its magnitude (a scalar) - a linear measurement</li>
<li>a force vector getting mapped to its magnitude</li>
</ul>
</li>
</ul>
<hr />
<h3 id="extra-notesdefs-to-categorize-later">Extra notes/defs to categorize later<a class="headerlink" href="#extra-notesdefs-to-categorize-later" title="Permanent link"></a></h3>
<p><strong>Dual Spaces and Dual Basis</strong><br />
* The dual space of V is the set of all linear functionals from V to \(\mathbb{F}\( , so : \(V^{v} = T: V \mapsto \mathbb{F} \( 
    - all such elements of dual space are linear functionals
- if dim(V) &lt; \( \infty\) =&gt; \( V \) and \( V^{v} \) are isomorphic
    + to show this is true, show that they have the same dimension
    + another way to show the isomorphism is to use the dual basis
        * linear extension theorem: says if you know what T does on basis vectos, you know what T does on every vector:
            * Let \(\mathbb{B} = { v_{1}....v_{n} }\(
            * enough to know what \(f(v_{1}),.....,f(v_{n})\( is 
            * usually hard to graphically represent linear transformations, but since the codoamin of all such linear functions are scalars, \(f: V \mapsto \mathbb{F}\(, you can draw a graph of your linear functionals (where your inputs end up.)</p>
<p><strong>Isomorphism</strong>
* mapppings that are injective and surjective (1:1 and onto) 
* <a href="https://www.youtube.com/watch?time_continue=6&amp;v=-Iww3p1-_bA&amp;feature=emb_logo">watch this</a>
* <strong>How to check if a mapping is an isomorphism between two vector spaces:</strong> 
    - a linear transformation \(T: V \mapsto W\) is one-to-one if:
        + \(T\) maps distinct vectors in V to distinct vectors in W
        + check if two distinct vectors in the domain give you two different vectors in the codomain when you apply the linear transformation to it.
    - it is onto if:
        * range(T) = W
        * so take any vector in W and you can find some vector in V that maps to it</p></article></body></html>